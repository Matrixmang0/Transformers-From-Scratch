{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* The objective of this assignments is to build the **Decoder** part of the Transformer architecture.\n",
        "* We will be using the **PyTorch** framework to implement the following components\n",
        "  * Decoder Layer that contains\n",
        "    * Multi-Head Masked Attention (MHMA) Module\n",
        "    * Multi-Head Cross Attention (MHMA) Module\n",
        "    * Position-wise Feed Forward Neural Network\n",
        "\n",
        "  * Implement CLM\n",
        "\n",
        "* **DO NOT** USE Built-in **TRANSFORMER LAYERS** as it affects the reproducibility.\n",
        "\n",
        "* You will be given with a configuration file that contains information on various hyperparameters such as embedding dimension, vocabulary size,number heads and so on\n",
        "\n",
        "* Use ReLU activation function and Stochastic Gradient Descent optimizer\n",
        "* Here are a list of helpful Pytorch functions (does not mean you have to use all of them) for this subsequent assignments\n",
        "  * [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch-matmul)\n",
        "  * [torch.bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "  * torch.swapdims\n",
        "  * torch.unsqueeze\n",
        "  * torch.squeeze\n",
        "  * torch.argmax\n",
        "  * [torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)\n",
        "  * [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "  * [torch.nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)\n",
        "  * torch.nn.Linear\n",
        "  * torch.nn.LayerNorm\n",
        "  * torch.nn.ModuleList\n",
        "  * torch.nn.Sequential\n",
        "  * [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "  \n",
        "* Important: Do not set any global seeds.\n",
        "\n",
        "* Helpful resources to get started with\n",
        "\n",
        " * [Andrej Karpathys Nano GPT](https://github.com/karpathy/nanoGPT)\n",
        " * [PyTorch Source code of Transformer Layer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "SLKgMhMp8wvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es7WIeXF3nrV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from  pprint import pprint\n",
        "from yaml import safe_load\n",
        "import copy\n",
        "import requests\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_url = \"https://raw.githubusercontent.com/Arunprakash-A/LLM-from-scratch-PyTorch/main/config_files/dec_config.yml\"\n",
        "response = requests.get(config_url)\n",
        "config = response.content.decode(\"utf-8\")\n",
        "config = safe_load(config)\n",
        "pprint(config)"
      ],
      "metadata": {
        "id": "de2_k13-3_gS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529e0669-21c7-4a0d-89b8-4d9272093f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': {'batch_size': 10, 'embed_dim': 32, 'seq_len': 8, 'vocab_size': 12},\n",
            " 'model': {'d_ff': 128,\n",
            "           'd_model': 32,\n",
            "           'dk': 4,\n",
            "           'dq': 4,\n",
            "           'dv': 4,\n",
            "           'n_heads': 8,\n",
            "           'n_layers': 6}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = config['input']['vocab_size']\n",
        "batch_size = config['input']['batch_size']\n",
        "seq_len = config['input']['seq_len']\n",
        "embed_dim = config['input']['embed_dim']\n",
        "dmodel = embed_dim\n",
        "dq = torch.tensor(config['model']['dq'])\n",
        "dk = torch.tensor(config['model']['dk'])\n",
        "dv = torch.tensor(config['model']['dv'])\n",
        "heads = torch.tensor(config['model']['n_heads'])\n",
        "d_ff = config['model']['d_ff']"
      ],
      "metadata": {
        "id": "kS7dciwb53M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input tokens"
      ],
      "metadata": {
        "id": "6uLkQpFA5-Gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Generate a raw_input ids (without any special tokens appended to it)\n",
        "\n",
        "* Since we will be using this as label after adding the special  \\<start\\> token, we use the variable name \"label_ids\"\n",
        "\n",
        "* Keep the size of the `label_ids=(bs,seq_len-1)` as we insert a special token ids in the next step"
      ],
      "metadata": {
        "id": "PfUUsznGp9Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://github.com/Arunprakash-A/LLM-from-scratch-PyTorch/raw/main/config_files/w2_input_tokens'\n",
        "r = requests.get(data_url)\n",
        "label_ids = torch.load(BytesIO(r.content))"
      ],
      "metadata": {
        "id": "I_D8hvdH5_1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4379456d-c273-4750-cfb5-7567c0098647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-8cc5628215f0>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  label_ids = torch.load(BytesIO(r.content))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNy9z5F5bM9x",
        "outputId": "5c28d2d6-fd56-45c5-fc9e-17fb1d8891a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 7,  8,  7,  7,  9,  2,  6],\n",
              "        [10,  1, 10,  5,  3,  6,  8],\n",
              "        [ 3,  4,  8,  2, 10, 10, 10],\n",
              "        [ 4, 10,  1,  3,  4,  9,  7],\n",
              "        [ 8,  4,  7,  3,  8, 10,  5],\n",
              "        [ 9,  1,  8,  5,  9,  9, 10],\n",
              "        [ 7,  3,  8,  2,  5,  1,  5],\n",
              "        [ 3,  3,  2,  1,  4,  1,  1],\n",
              "        [10,  9,  9,  9,  6,  9,  2],\n",
              "        [ 3,  6,  6,  3,  5,  4,  5]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let the first token_id be be a special `[start]` token (mapped to integer 0)\n",
        "* If label_ids=$\\begin{bmatrix}1&2\\\\3&4 \\end{bmatrix}$, then we modify it as $\\begin{bmatrix}0&1&2\\\\0&3&4 \\end{bmatrix}$"
      ],
      "metadata": {
        "id": "nxmtUf5j6JJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "begin_token = torch.zeros(label_ids.shape[0], 1, dtype=int)\n",
        "\n",
        "token_ids = torch.cat((begin_token, label_ids), dim=1) # the first column of token_ids should be zeros and the rest of the columns come from label_ids"
      ],
      "metadata": {
        "id": "ADbTWVVC6fu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGyxwg-ma0Ij",
        "outputId": "99db16be-1c19-4786-c51c-a559637e5321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  7,  8,  7,  7,  9,  2,  6],\n",
              "        [ 0, 10,  1, 10,  5,  3,  6,  8],\n",
              "        [ 0,  3,  4,  8,  2, 10, 10, 10],\n",
              "        [ 0,  4, 10,  1,  3,  4,  9,  7],\n",
              "        [ 0,  8,  4,  7,  3,  8, 10,  5],\n",
              "        [ 0,  9,  1,  8,  5,  9,  9, 10],\n",
              "        [ 0,  7,  3,  8,  2,  5,  1,  5],\n",
              "        [ 0,  3,  3,  2,  1,  4,  1,  1],\n",
              "        [ 0, 10,  9,  9,  9,  6,  9,  2],\n",
              "        [ 0,  3,  6,  6,  3,  5,  4,  5]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the following components of a decoder layer\n",
        "\n",
        " * Multi-head Masked Attention (MHMA)\n",
        " * Multi-head Cross Attention (MHCA)\n",
        " * Postion-wise FFN\n",
        "\n",
        "* Randomly initialize the parameters using normal distribution with the following seed values\n",
        "  * $W_Q:$(seed=43)\n",
        "  * $W_K:$(seed=44)\n",
        "  * $W_V:$(seed=45)\n",
        "  * $W_O:$(seed=46)\n",
        "\n",
        "* Remember that, Multi-head cross atention takes two represnetation. One is the encoder output and the other one is the output from masked attetnion sub-layer.\n",
        "\n",
        "* However, in this assignment, we will fix it to a random matrix."
      ],
      "metadata": {
        "id": "nrDQbLIF334F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHCA(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,heads):\n",
        "    super(MHCA,self).__init__()\n",
        "\n",
        "    self.dmodel = dmodel\n",
        "    self.dq = dq\n",
        "    self.dk = dk\n",
        "    self.dv = dv\n",
        "    self.heads = heads\n",
        "\n",
        "    torch.manual_seed(43)\n",
        "    self.W_Q = nn.Parameter(torch.randn(dq * heads, dmodel))\n",
        "\n",
        "    torch.manual_seed(44)\n",
        "    self.W_K = nn.Parameter(torch.randn(dk * heads, dmodel))\n",
        "\n",
        "    torch.manual_seed(45)\n",
        "    self.W_V = nn.Parameter(torch.randn(dv * heads, dmodel))\n",
        "\n",
        "    torch.manual_seed(46)\n",
        "    self.W_O = nn.Parameter(torch.randn(dmodel, dv * heads))\n",
        "\n",
        "\n",
        "  def forward(self, dec_query, enc_key, enc_value):\n",
        "\n",
        "    Q = torch.matmul(dec_query, self.W_Q.T)\n",
        "    K = torch.matmul(enc_key, self.W_K.T)\n",
        "    V = torch.matmul(enc_value, self.W_V.T)\n",
        "\n",
        "    batch_size = Q.shape[0]\n",
        "    seq_len = Q.shape[1]\n",
        "\n",
        "    Q = Q.view(batch_size, seq_len, self.heads, self.dq).transpose(1, 2)\n",
        "    K = K.view(batch_size, seq_len, self.heads, self.dk).transpose(1, 2)\n",
        "    V = V.view(batch_size, seq_len, self.heads, self.dv).transpose(1, 2)\n",
        "\n",
        "    interim_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(self.dk)\n",
        "    interim_scores_sm = F.softmax(interim_scores, dim=-1)\n",
        "\n",
        "    attn_scores = torch.matmul(interim_scores_sm, V)\n",
        "\n",
        "    attn_scores_T = attn_scores.transpose(1, 2).contiguous().view(batch_size, seq_len, self.heads * self.dv)\n",
        "\n",
        "    out = torch.matmul(attn_scores_T, self.W_O.T)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "fkRKwCzY_uOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By default, `mask=None`. Therefore, create and apply the mask while computing the attention scores\n"
      ],
      "metadata": {
        "id": "3axiToa8AiTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHMA(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,heads,mask=None):\n",
        "    super(MHMA,self).__init__()\n",
        "\n",
        "    self.dmodel = dmodel\n",
        "    self.dq = dq\n",
        "    self.dk = dk\n",
        "    self.dv = dv\n",
        "    self.heads = heads\n",
        "    self.mask = mask\n",
        "\n",
        "    torch.manual_seed(43)\n",
        "    self.W_Q = nn.Parameter(torch.randn(dq * heads, dmodel))\n",
        "\n",
        "    torch.manual_seed(44)\n",
        "    self.W_K = nn.Parameter(torch.randn(dk * heads, dmodel))\n",
        "\n",
        "    torch.manual_seed(45)\n",
        "    self.W_V = nn.Parameter(torch.randn(dv * heads, dmodel))\n",
        "\n",
        "    torch.manual_seed(46)\n",
        "    self.W_O = nn.Parameter(torch.randn(dmodel, dv * heads))\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    Q = torch.matmul(x, self.W_Q.T)\n",
        "    K = torch.matmul(x, self.W_K.T)\n",
        "    V = torch.matmul(x, self.W_V.T)\n",
        "\n",
        "    batch_size = Q.shape[0]\n",
        "    seq_len = Q.shape[1]\n",
        "\n",
        "    Q = Q.view(batch_size, seq_len, self.heads, self.dq).transpose(1, 2)\n",
        "    K = K.view(batch_size, seq_len, self.heads, self.dk).transpose(1, 2)\n",
        "    V = V.view(batch_size, seq_len, self.heads, self.dv).transpose(1, 2)\n",
        "\n",
        "    interim_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(self.dk)\n",
        "\n",
        "    if self.mask==None:\n",
        "      self.mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1)\n",
        "      self.mask = self.mask == 1\n",
        "\n",
        "    masked_scores = interim_scores.masked_fill(self.mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "    masked_scores_sm = F.softmax(masked_scores, dim=-1)\n",
        "\n",
        "    attn_scores = torch.matmul(masked_scores_sm, V)\n",
        "\n",
        "    out = attn_scores.transpose(1, 2).contiguous().view(batch_size, seq_len, self.heads * self.dv)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "2URLKYHT8i1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implement the FFN and OutputLayer modules (same as the one you implemented for encoder)"
      ],
      "metadata": {
        "id": "HkWY1xuzbV1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,d_ff):\n",
        "    super(FFN,self).__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(dmodel, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, dmodel)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.linear1(x)\n",
        "    x = F.relu(x)\n",
        "    out = self.linear2(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "WriFqQWHAGZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,vocab_size):\n",
        "    super(OutputLayer,self).__init__()\n",
        "\n",
        "    self.linear = nn.Linear(dmodel, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out = self.linear(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "sZ8mE5xQbzh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implement the final decoder layer."
      ],
      "metadata": {
        "id": "htg7Hx-Kb1_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,d_ff,heads,mask=None):\n",
        "    super(DecoderLayer,self).__init__()\n",
        "    self.mhma = MHMA(dmodel,dq,dk,dv,heads,mask=mask)\n",
        "    self.mhca = MHCA(dmodel,dq,dk,dv,heads)\n",
        "    self.layer_norm_mhma = torch.nn.LayerNorm(dmodel)\n",
        "    self.layer_norm_mhca = torch.nn.LayerNorm(dmodel)\n",
        "    self.layer_norm_ffn = torch.nn.LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,d_ff)\n",
        "\n",
        "  def forward(self, x, enc_output):\n",
        "\n",
        "    mhma_output = self.mhma(x)\n",
        "    x = self.layer_norm_mhma(x + mhma_output)\n",
        "\n",
        "    mhca_output = self.mhca(x, enc_output, enc_output)\n",
        "    x = self.layer_norm_mhca(x + mhca_output)\n",
        "\n",
        "    ffn_output = self.ffn(x)\n",
        "    out = self.layer_norm_ffn(x + ffn_output)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "y3KrOhIj_1YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create an embedding layer that takes in token_ids and return embeddings for the token_ids\n",
        "\n",
        " * Use seed value: 70"
      ],
      "metadata": {
        "id": "0on1cUNEcEt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,embed_dim):\n",
        "    super(Embed,self).__init__()\n",
        "\n",
        "    torch.manual_seed(70)\n",
        "    self.embed= nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.embed(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "0a--wC-_Tf_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder\n",
        "\n",
        " * Implement the decoder that has `num_layers` decoder layers"
      ],
      "metadata": {
        "id": "Xr3nTQP5d2zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,dmodel,dq,dk,dv,d_ff,heads,mask,num_layers=1):\n",
        "    super(Decoder,self).__init__()\n",
        "\n",
        "    self.embed_lookup = Embed(vocab_size, dmodel)\n",
        "\n",
        "    decoder_layer = DecoderLayer(dmodel, dq, dk, dv, d_ff, heads, mask)\n",
        "    self.dec_layers = nn.ModuleList([copy.deepcopy(decoder_layer) for i in range(num_layers)])\n",
        "\n",
        "    self.output_layer = OutputLayer(dmodel, vocab_size)\n",
        "\n",
        "  def forward(self,enc_rep,tar_token_ids):\n",
        "\n",
        "    x = self.embed_lookup(tar_token_ids)\n",
        "\n",
        "    for dec_layer in self.dec_layers:\n",
        "        x = dec_layer(x, enc_rep)\n",
        "\n",
        "    out = self.output_layer(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "SYRrxMkpBzAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Representation from encoder\n",
        "\n",
        " * Since all the decoder layers require the representation from the encoder to compute cross attention, we are going to feed in the random values (Note, it does not require gradient during training)"
      ],
      "metadata": {
        "id": "lyZYR7NkCYh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_rep = torch.randn(size=(batch_size,seq_len,embed_dim),generator=torch.random.manual_seed(10))"
      ],
      "metadata": {
        "id": "-rOysYMjB-af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the model"
      ],
      "metadata": {
        "id": "tpLKJVXnGbbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder(vocab_size,dmodel,dq,dk,dv,d_ff,heads,mask=None)"
      ],
      "metadata": {
        "id": "aeG9CjI_GcsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "aebVyW-9LhcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(enc_rep,tar_token_ids,label_ids,epochs=1000):\n",
        "  loss_trace = []\n",
        "  for epoch in range(epochs):\n",
        "    out = model(enc_rep,tar_token_ids)\n",
        "    out = out.view(-1, vocab_size)\n",
        "\n",
        "    target = tar_token_ids.view(-1)\n",
        "\n",
        "    loss = criterion(out, target)\n",
        "    loss_trace.append(loss.item())\n",
        "\n",
        "    if (epoch+1)%100 == 0:\n",
        "      print(\"Epoch :\", epoch, \"Loss :\", loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "8npNWRJ7tWRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train the model for 1000 epochs"
      ],
      "metadata": {
        "id": "lXLdvUVKgU3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(enc_rep,token_ids,label_ids,1000)"
      ],
      "metadata": {
        "id": "svrd7-yHtpme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cefb44-96ef-4a63-b4d2-a82700f3b783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 99 Loss : 1.9622176885604858\n",
            "Epoch : 199 Loss : 1.5867887735366821\n",
            "Epoch : 299 Loss : 1.302473783493042\n",
            "Epoch : 399 Loss : 1.0674269199371338\n",
            "Epoch : 499 Loss : 0.877951443195343\n",
            "Epoch : 599 Loss : 0.7048269510269165\n",
            "Epoch : 699 Loss : 0.5797830820083618\n",
            "Epoch : 799 Loss : 0.4468874931335449\n",
            "Epoch : 899 Loss : 0.35477375984191895\n",
            "Epoch : 999 Loss : 0.28072211146354675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  predictions = torch.argmax(model(enc_rep,token_ids),dim=-1)"
      ],
      "metadata": {
        "id": "M6PLInPW0JIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The loss will be around 0.17 after 1000 epochs"
      ],
      "metadata": {
        "id": "TsA-osXehjEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of correct predictions\n",
        "print(torch.count_nonzero(label_ids==predictions[:,1:]))"
      ],
      "metadata": {
        "id": "hbnfyb5thxCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa72b1f7-8f47-4c45-9657-4f36a1373e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(69)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The number of correct predictions is close to 66"
      ],
      "metadata": {
        "id": "UXTPr2pd6tfo"
      }
    }
  ]
}