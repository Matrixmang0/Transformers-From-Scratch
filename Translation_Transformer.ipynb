{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* In this assignment you will be using the entire transformer architecture for a translation task.\n",
        "* we will just be using one encoder layer and one decoder layer\n",
        "* You can copy the encoder and decoder modules from the previous assignments. You are going to translate a few sentences from **English to Tamil**\n",
        "  * Source language: English\n",
        "  * Target language: Tamil\n",
        "\n",
        "* You may experiment with a target language of your choice for checking the impelementation. (You may use google translate for that)\n",
        "\n",
        "* We need to install torchdata and torchtext (which take about 3 minutes to finish installing) for tokenizing the text.\n",
        "* We already defined useful functions for the tokenization of texts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HuEecgDwXV4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata==0.6.0 # to be compatible with torch 2.0\n",
        "!pip install portalocker==2.0.0\n",
        "!pip install -U torchtext==0.15.1"
      ],
      "metadata": {
        "id": "m1TwVG-9cQmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2aa666-bf34-4b29-cf9a-f3ed7813aeac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchdata==0.6.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchdata==0.6.0) (1.3.0)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: torchtext==0.15.1 in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's import all required libraries"
      ],
      "metadata": {
        "id": "qiGXHBiwcvI1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4nHBd0CHVX5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "#text lib\n",
        "import torchtext\n",
        "\n",
        "# tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "#build vocabulary\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# get input_ids (numericalization)\n",
        "from torchtext.transforms import VocabTransform, LabelToIndex\n",
        "\n",
        "# get embeddings\n",
        "from torch.nn import Embedding\n",
        "\n",
        "from  pprint import pprint\n",
        "from yaml import safe_load\n",
        "import copy\n",
        "import numpy as np\n",
        "import requests\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data"
      ],
      "metadata": {
        "id": "1UbaTNbHac1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Source and target text"
      ],
      "metadata": {
        "id": "8O1Qu_Z8pBZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_text = \"\"\"The most famous ruler of ancient India was Emperor Ashoka.\n",
        "It was during his period that Buddhism spread to different parts of Asia.\n",
        "Ashoka gave up war after seeing many people grieving death after the Kalinga war.\n",
        "He embraced Buddhism and then devoted his life to spread the message of peace and dharma.\n",
        "His service for the cause of public good was exemplary.\n",
        "He was the first ruler to give up war after victory.\n",
        "He was the first to build hospitals for animals.\n",
        "He was the first to lay roads.\"\"\""
      ],
      "metadata": {
        "id": "9rv6jV3yaf1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_text = \"\"\"பண்டைய இந்திய அரசர்களில் பேரும் புகழும் பெற்ற அரசர் அசோகர் ஆவார்.\n",
        "இவரது ஆட்சியில் தான் புத்த மதம் ஆசியாவின் பல்வேறு பகுதிகளுக்குப் பரவியது.\n",
        "கலிங்கப் போருக்குப் பின் பல உயிர்கள் மடிவதைக் கண்டு வருந்தி, போர் தொடுப்பதைக் கைவிட்டார்.\n",
        "அதற்குப் பிறகு புத்த சமயத்தைத் தழுவி, அமைதியையும் அறத்தையும் பரப்புவதற்காகத் தன் வாழ்வையே அர்ப்பணித்தார்.\n",
        "பொதுமக்களுக்கு அவர் ஆற்றிய சேவை முன் மாதிரியாக விளங்கியது.\n",
        "வெற்றிக்குப் பின் போரைத் துறந்த முதல் அரசர் அசோகர்தான்.\n",
        "உலகிலேயே முதன்முதலாக விலங்குகளுக்கும் தனியே மருத்துவமனை அமைத்துத் தந்தவரும் அசோகரே ஆவார்.\n",
        " இன்றும் அவர் உருவாக்கிய சாலைகளை நாம் பயன்படுத்திக்கொண்டு இருக்கிறோம்.\"\"\""
      ],
      "metadata": {
        "id": "fFXiagnva88E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tokenize and build vocabulary using a simple tokenization algorithm"
      ],
      "metadata": {
        "id": "h7vjJPzJpNrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_len(seq):\n",
        "  return len(seq.strip('').split(' '))\n",
        "\n",
        "# check the maximum length of the src and target seq to decide the context length of encdoer and decoder\n",
        "src_raw_seq = src_text.strip('').split('\\n')\n",
        "src_max_seq_len =max(list(map(seq_len,src_raw_seq)))\n",
        "print('Source max_seq_length:  ',src_max_seq_len)\n",
        "\n",
        "\n",
        "tar_raw_seq = tar_text.strip('').split('\\n')\n",
        "tar_max_seq_len =max(list(map(seq_len,tar_raw_seq)))\n",
        "print('Target max_seq_length: ',tar_max_seq_len)"
      ],
      "metadata": {
        "id": "3renrPd6fh01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211e1d0a-1523-483f-f8e8-4f1d603929ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source max_seq_length:   16\n",
            "Target max_seq_length:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We encourage you to go through the code given below to understand the typical functionalities of Tokenizer object (If you want, you can skip)"
      ],
      "metadata": {
        "id": "4xhQw4aM66_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "class Tokenizer(object):\n",
        "\n",
        "  def __init__(self,text):\n",
        "    self.text = text\n",
        "    self.word_tokenizer = self.word_tokenizer\n",
        "    self.vocab_size = None\n",
        "    self.vocab = None\n",
        "\n",
        "  @staticmethod\n",
        "  def word_tokenizer(seq):\n",
        "    return seq.strip('').split(' ')\n",
        "\n",
        "  def get_tokens(self):\n",
        "    for sentence in self.text.strip().split('\\n'):\n",
        "      yield self.word_tokenizer(sentence)\n",
        "\n",
        "  def build_vocab(self):\n",
        "    self.vocab = build_vocab_from_iterator(self.get_tokens(),\n",
        "                                  min_freq=1,specials=['<pad>','<start>','<end>','<unk>'])\n",
        "    self.vocab.set_default_index(self.vocab['<unk>']) # index of OOV\n",
        "    self.vocab_size = len(self.vocab)\n",
        "    return self.vocab\n",
        "\n",
        "  def encode(self,sentence):\n",
        "    v = self.build_vocab()\n",
        "    vt = VocabTransform(v)\n",
        "    token_ids = vt(self.word_tokenizer(sentence))\n",
        "    # add special tokens\n",
        "    token_ids.insert(0,v.vocab.get_stoi()['<start>'])\n",
        "    token_ids.append(v.vocab.get_stoi()['<end>']) # <end>:2\n",
        "    return torch.tensor(token_ids,dtype=torch.int64)\n",
        "\n",
        "  def decode(self,ids):\n",
        "    v = self.build_vocab()\n",
        "    list_ids = ids.tolist()\n",
        "    tokens = [v.vocab.get_itos()[id] for id in list_ids]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "  def encode_batch(self,batch_size,max_seq_len):\n",
        "    batch_data = torch.zeros(size=(batch_size,max_seq_len+2)) # +2 for special tokens\n",
        "    for i,sentence in enumerate(self.text.strip('').split('\\n')):\n",
        "      token_ids = self.encode(sentence)\n",
        "      batch_data[i,0:len(token_ids)] = token_ids\n",
        "    return batch_data.type(dtype=torch.int64)\n",
        "\n"
      ],
      "metadata": {
        "id": "b2QLspwihNSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It is always go to check the implementation"
      ],
      "metadata": {
        "id": "tIiCA9uppzMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8"
      ],
      "metadata": {
        "id": "BPWNsuaolNcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokenizer = Tokenizer(src_text)\n",
        "print(src_tokenizer.encode('The most famous ruler of ancient India was Emperor Ashoka.'))\n",
        "print(src_tokenizer.encode_batch(batch_size,src_max_seq_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV0wYxrFkSzr",
        "outputId": "a020c261-fef8-47c7-d224-a09e1d7def5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1, 27, 49, 39, 15,  8, 28, 24,  5, 22, 20,  2])\n",
            "tensor([[ 1, 27, 49, 39, 15,  8, 28, 24,  5, 22, 20,  2,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 25,  5, 36, 14, 53, 58, 11, 16,  6, 35, 50,  8, 21,  2,  0,  0,  0],\n",
            "        [ 1, 19, 40, 17, 18,  9, 56, 47, 52, 43, 32,  9,  4, 26, 61,  2,  0,  0],\n",
            "        [ 1,  7, 37, 11, 12, 59, 33, 14, 46,  6, 16,  4, 48,  8, 51, 12, 34,  2],\n",
            "        [ 1, 23, 57, 13,  4, 31,  8, 54, 42,  5, 38,  2,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  7,  5,  4, 10, 15,  6, 41, 17, 18,  9, 60,  2,  0,  0,  0,  0,  0],\n",
            "        [ 1,  7,  5,  4, 10,  6, 30, 44, 13, 29,  2,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  7,  5,  4, 10,  6, 45, 55,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tar_tokenizer = Tokenizer(tar_text)\n",
        "print(tar_tokenizer.encode('பண்டைய இந்திய அரசர்களில் பேரும் புகழும் பெற்ற அரசர் அசோகர் ஆவார்.'))\n",
        "print(tar_tokenizer.encode_batch(batch_size,tar_max_seq_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZBhYD7MxzpR",
        "outputId": "c8dea4ad-52cb-4637-fe0c-015bf8f1aa27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1, 44, 22, 16, 53, 51, 52,  4, 11,  6,  2])\n",
            "tensor([[ 1, 44, 22, 16, 53, 51, 52,  4, 11,  6,  2,  0,  0],\n",
            "        [ 1, 25, 20, 39,  8, 59, 19, 49, 43, 47,  2,  0,  0],\n",
            "        [ 1, 30, 55,  7, 48, 26, 58, 29, 65, 57, 41, 31,  2],\n",
            "        [ 1, 13, 50,  8, 32, 38, 14, 18, 46, 37, 66, 17,  2],\n",
            "        [ 1, 54,  5, 21, 34, 64, 61, 68,  2,  0,  0,  0,  0],\n",
            "        [ 1, 69,  7, 56, 40, 63,  4, 12,  2,  0,  0,  0,  0],\n",
            "        [ 1, 28, 62, 67, 36, 60, 15, 35, 10,  6,  2,  0,  0],\n",
            "        [ 1,  9, 23,  5, 27, 33, 42, 45, 24,  2,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's load the token ids of the words in the sentences of source and target languages"
      ],
      "metadata": {
        "id": "FNVpiIqDKp0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = src_tokenizer.encode_batch(batch_size,src_max_seq_len)\n",
        "y = tar_tokenizer.encode_batch(batch_size,tar_max_seq_len)"
      ],
      "metadata": {
        "id": "wQKgFX1xLtC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we have appended zeros to sentences that are shorter than max-seq-len\n",
        "* We have to ignore computing loss over those padded tokens\n",
        "* You have to take care of that in the cell below"
      ],
      "metadata": {
        "id": "DjE9mlNOxQlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = torch.where(y[:, 1:] == src_tokenizer.vocab['<pad>'], 0, y[:, 1:])\n",
        "label = torch.cat([label, torch.zeros((label.size(0), 1), dtype=label.dtype, device=label.device)], dim=1)\n",
        "label = label.int()\n",
        "label"
      ],
      "metadata": {
        "id": "z-cIRfUeLLpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4ea482-9d01-468b-f10b-a4a149c5fab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[44, 22, 16, 53, 51, 52,  4, 11,  6,  2,  0,  0,  0],\n",
              "        [25, 20, 39,  8, 59, 19, 49, 43, 47,  2,  0,  0,  0],\n",
              "        [30, 55,  7, 48, 26, 58, 29, 65, 57, 41, 31,  2,  0],\n",
              "        [13, 50,  8, 32, 38, 14, 18, 46, 37, 66, 17,  2,  0],\n",
              "        [54,  5, 21, 34, 64, 61, 68,  2,  0,  0,  0,  0,  0],\n",
              "        [69,  7, 56, 40, 63,  4, 12,  2,  0,  0,  0,  0,  0],\n",
              "        [28, 62, 67, 36, 60, 15, 35, 10,  6,  2,  0,  0,  0],\n",
              "        [ 9, 23,  5, 27, 33, 42, 45, 24,  2,  0,  0,  0,  0]],\n",
              "       dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Define the context lengths for encoder and decoder"
      ],
      "metadata": {
        "id": "k0i4jRhh5U95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_ctxt_len = src_max_seq_len+2\n",
        "dec_ctxt_len = tar_max_seq_len+2"
      ],
      "metadata": {
        "id": "Z6YT0qC-MEeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load configuration file"
      ],
      "metadata": {
        "id": "LxpOb7WXJuPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_url = \"https://raw.githubusercontent.com/Arunprakash-A/LLM-from-scratch-PyTorch/main/config_files/enc_config.yml\"\n",
        "response = requests.get(config_url)\n",
        "config = response.content.decode(\"utf-8\")\n",
        "config = safe_load(config)\n",
        "pprint(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwp2oxceUguz",
        "outputId": "959b7f67-2d2d-4e7b-d845-5877f82b2750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': {'batch_size': 10, 'embed_dim': 32, 'seq_len': 8, 'vocab_size': 10},\n",
            " 'model': {'d_ff': 128,\n",
            "           'd_model': 32,\n",
            "           'dk': 4,\n",
            "           'dq': 4,\n",
            "           'dv': 4,\n",
            "           'n_heads': 8,\n",
            "           'n_layers': 6}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size =src_tokenizer.vocab_size\n",
        "batch_size = x.shape[0]\n",
        "embed_dim = config['input']['embed_dim']"
      ],
      "metadata": {
        "id": "VdNVN3GUUqi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dq = torch.tensor(config['model']['dq'])\n",
        "dk = torch.tensor(config['model']['dk'])\n",
        "dv = torch.tensor(config['model']['dv'])\n",
        "dmodel = embed_dim\n",
        "heads = torch.tensor(config['model']['n_heads'])\n",
        "d_ff = config['model']['d_ff']"
      ],
      "metadata": {
        "id": "UPKo8KPrXDIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_url = \"https://raw.githubusercontent.com/Arunprakash-A/LLM-from-scratch-PyTorch/main/config_files/dec_config.yml\"\n",
        "response = requests.get(config_url)\n",
        "config = response.content.decode(\"utf-8\")\n",
        "config = safe_load(config)\n",
        "pprint(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFJh4tsEJva-",
        "outputId": "ff402139-fc06-47d7-8974-1caf8020eca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': {'batch_size': 10, 'embed_dim': 32, 'seq_len': 8, 'vocab_size': 12},\n",
            " 'model': {'d_ff': 128,\n",
            "           'd_model': 32,\n",
            "           'dk': 4,\n",
            "           'dq': 4,\n",
            "           'dv': 4,\n",
            "           'n_heads': 8,\n",
            "           'n_layers': 6}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab_size = tar_tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "Ijfm3gfDPWs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n",
        "\n",
        " * You can copy paste the required code from the previous assignments"
      ],
      "metadata": {
        "id": "tjIYp4I177LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "    def __init__(self, dmodel, dq, dk, dv, heads):\n",
        "        super(MHA,self).__init__()\n",
        "        self.dk = dk\n",
        "        torch.manual_seed(43)\n",
        "        self.WQ = nn.Parameter(torch.randn(heads, dmodel, dq))\n",
        "        torch.manual_seed(44)\n",
        "        self.WK = nn.Parameter(torch.randn(heads, dmodel, dk))\n",
        "        torch.manual_seed(45)\n",
        "        self.WV = nn.Parameter(torch.randn(heads, dmodel, dv))\n",
        "        torch.manual_seed(46)\n",
        "        self.WO = nn.Parameter(torch.randn(dmodel, dmodel))\n",
        "\n",
        "\n",
        "    def forward(self, H):\n",
        "\n",
        "        BS, T, _ = H.size()\n",
        "\n",
        "        Q = torch.matmul(H.unsqueeze(1), self.WQ)\n",
        "        K = torch.matmul(H.unsqueeze(1), self.WK)\n",
        "        V = torch.matmul(H.unsqueeze(1), self.WV)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(2, 3)) / np.sqrt(self.dk)\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        attn_out = torch.matmul(attn_weights, V)\n",
        "        attn_out = attn_out.permute(0,2,1,3).contiguous().view(BS, T, -1)\n",
        "        out = torch.matmul(attn_out, self.WO)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, dmodel, d_ff, layer=0):\n",
        "        super(FFN, self).__init__()\n",
        "        self.W1 = nn.Parameter(torch.randn((dmodel, d_ff), generator = torch.manual_seed(47)))\n",
        "        self.b1 = nn.Parameter(torch.randn((d_ff), generator = torch.manual_seed(48)))\n",
        "        self.W2 = nn.Parameter(torch.randn((d_ff, dmodel), generator = torch.manual_seed(49)))\n",
        "        self.b2 = nn.Parameter(torch.randn((dmodel), generator = torch.manual_seed(50)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.matmul(x, self.W1) + self.b1\n",
        "        out = nn.ReLU()(out)\n",
        "        out = torch.matmul(out, self.W2) + self.b2\n",
        "        return out\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,d_ff,heads):\n",
        "    super(EncoderLayer,self).__init__()\n",
        "    self.mha = MHA(dmodel,dq,dk,dv,heads)\n",
        "    self.layer_norm_mha = torch.nn.LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,d_ff, layer=0)\n",
        "    self.layer_norm_ffn = torch.nn.LayerNorm(dmodel)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.layer_norm_mha(self.mha(x)+x)\n",
        "    out = self.layer_norm_ffn(self.ffn(out)+out)\n",
        "    return out\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, dq, dk, dv, d_ff, heads, num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        base_layer = EncoderLayer(dmodel=embed_dim, dq=dq, dk=dk, dv=dv, d_ff=d_ff, heads=heads)\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            copy.deepcopy(base_layer) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.encoder_layers:\n",
        "            out = layer(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "MVxV2ROQ8Aky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "dpx5HW_bXHWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHMA(nn.Module):\n",
        "    def __init__(self, num_heads, model_dim, query_dim, key_dim, value_dim):\n",
        "        super(MHMA, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.query_dim = query_dim\n",
        "\n",
        "        self.query_weights = nn.Parameter(torch.randn((num_heads, model_dim, query_dim), generator=torch.manual_seed(51)))\n",
        "        self.key_weights = nn.Parameter(torch.randn((num_heads, model_dim, key_dim), generator=torch.manual_seed(52)))\n",
        "        self.value_weights = nn.Parameter(torch.randn((num_heads, model_dim, value_dim), generator=torch.manual_seed(53)))\n",
        "        self.output_weights = nn.Parameter(torch.randn((model_dim, model_dim), generator=torch.manual_seed(54)))\n",
        "\n",
        "        nn.init.xavier_normal_(self.query_weights)\n",
        "        nn.init.xavier_normal_(self.key_weights)\n",
        "        nn.init.xavier_normal_(self.value_weights)\n",
        "        nn.init.xavier_normal_(self.output_weights)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        batch_size, seq_len, _ = query.shape\n",
        "        query = torch.einsum('BSM, HMQ -> BHSQ', query, self.query_weights)\n",
        "        key = torch.einsum('BSM, HMK -> BHSK', key, self.key_weights)\n",
        "        value = torch.einsum('BSM, HMV -> BHSV', value, self.value_weights)\n",
        "\n",
        "        attention_scores = torch.matmul(\n",
        "            F.softmax((torch.matmul(query, key.transpose(-2, -1)) + mask) / math.sqrt(self.query_dim), dim=-1), value\n",
        "        )\n",
        "        combined_attention = attention_scores.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, -1)\n",
        "        output = torch.matmul(combined_attention, self.output_weights)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class MHCA(nn.Module):\n",
        "    def __init__(self, num_heads, model_dim, query_dim, key_dim, value_dim):\n",
        "        super(MHCA, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.query_dim = query_dim\n",
        "\n",
        "        self.query_weights = nn.Parameter(torch.randn((num_heads, model_dim, query_dim), generator=torch.manual_seed(55)))\n",
        "        self.key_weights = nn.Parameter(torch.randn((num_heads, model_dim, key_dim), generator=torch.manual_seed(56)))\n",
        "        self.value_weights = nn.Parameter(torch.randn((num_heads, model_dim, value_dim), generator=torch.manual_seed(57)))\n",
        "        self.output_weights = nn.Parameter(torch.randn((model_dim, model_dim), generator=torch.manual_seed(58)))\n",
        "\n",
        "        nn.init.xavier_normal_(self.query_weights)\n",
        "        nn.init.xavier_normal_(self.key_weights)\n",
        "        nn.init.xavier_normal_(self.value_weights)\n",
        "        nn.init.xavier_normal_(self.output_weights)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        batch_size, seq_len, _ = query.shape\n",
        "        query = torch.einsum('BSM, HMQ -> BHSQ', query, self.query_weights)\n",
        "        key = torch.einsum('BSM, HMK -> BHSK', key, self.key_weights)\n",
        "        value = torch.einsum('BSM, HMV -> BHSV', value, self.value_weights)\n",
        "\n",
        "        attention_scores = torch.matmul(\n",
        "            F.softmax(torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.query_dim), dim=-1), value\n",
        "        )\n",
        "        combined_attention = attention_scores.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, -1)\n",
        "        output = torch.matmul(combined_attention, self.output_weights)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, model_dim, query_dim, key_dim, value_dim, ff_dim, num_heads, mask):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MHMA(num_heads, model_dim, query_dim, key_dim, value_dim)\n",
        "        self.cross_attention = MHCA(num_heads, model_dim, query_dim, key_dim, value_dim)\n",
        "        self.feed_forward = FFN(model_dim, ff_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
        "        self.layer_norm3 = nn.LayerNorm(model_dim)\n",
        "        self.mask = mask\n",
        "\n",
        "    def forward(self, encoder_output, decoder_input):\n",
        "        attn_output = self.self_attention(decoder_input, decoder_input, decoder_input, self.mask)\n",
        "        attn_output = self.layer_norm1(attn_output + decoder_input)\n",
        "\n",
        "        cross_attn_output = self.cross_attention(attn_output, encoder_output, encoder_output)\n",
        "        cross_attn_output = self.layer_norm2(cross_attn_output + attn_output)\n",
        "\n",
        "        ff_output = self.feed_forward(cross_attn_output)\n",
        "        output = self.layer_norm3(ff_output + cross_attn_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, model_dim, query_dim, key_dim, value_dim, ff_dim, num_heads, mask, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [copy.deepcopy(DecoderLayer(model_dim, query_dim, key_dim, value_dim, ff_dim, num_heads, mask))\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "        self.mask = mask\n",
        "\n",
        "    def forward(self, encoder_output, decoder_input):\n",
        "        output = decoder_input\n",
        "        for layer in self.decoder_layers:\n",
        "            output = layer(encoder_output, output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class OutputLayer(nn.Module):\n",
        "    def __init__(self, model_dim, vocab_size):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.randn((model_dim, vocab_size), generator=torch.manual_seed(10)))\n",
        "        self.bias = nn.Parameter(torch.randn((vocab_size), generator=torch.manual_seed(10)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.matmul(x, self.weight) + self.bias\n",
        "        return output"
      ],
      "metadata": {
        "id": "jDZSDFHnX5MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Embedding\n",
        "\n",
        " * You may take the code directly from any source."
      ],
      "metadata": {
        "id": "dtqZMexg-Ov0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,d_model, max_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.arange(0, max_len*d_model).reshape(max_len, d_model).float()\n",
        "        pe[0:,0::2] = torch.sin(torch.exp(torch.log(pe[0:, 0::2]%d_model)-(2*(pe[0:, 0::2]//d_model)/d_model)*torch.log(torch.tensor(10000))))\n",
        "        pe[0:,1::2] = torch.cos(torch.exp(torch.log(pe[0:, 1::2]%d_model)-(2*(pe[0:, 1::2]//d_model)/d_model)*torch.log(torch.tensor(10000))))\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ehcB8MeD-Rbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate target mask\n",
        "\n",
        "  * We will be passing the causal mask for the decoder layer as one of its arguments"
      ],
      "metadata": {
        "id": "Q5BDOOj_cJlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = (torch.triu(torch.ones(dec_ctxt_len,dec_ctxt_len)) == 1).transpose(0,1)\n",
        "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngAlmGaccFva",
        "outputId": "e3ee5705-5d6d-44f6-d17a-7e05503b3290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "DyXhnsw4SPeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self,src_vocab_size,tar_vocab_size,src_seq_len,tar_seq_len,dmodel,dq,dk,dv,d_ff,heads,target_mask,num_layers=1):\n",
        "      super(Transformer,self).__init__()\n",
        "      self.src_embeddings = nn.Embedding(src_vocab_size,embed_dim)\n",
        "      self.tar_embeddings = nn.Embedding(tar_vocab_size,embed_dim)\n",
        "      self.src_pos_embeddings = PositionalEncoding(dmodel, src_seq_len)\n",
        "      self.tar_pos_embeddings = PositionalEncoding(dmodel, tar_seq_len)\n",
        "      self.encoder = Encoder(src_vocab_size,dmodel,dq,dk,dv,d_ff,heads,num_layers)\n",
        "      self.decoder = Decoder(tar_vocab_size,dmodel,dq,dk,dv,d_ff,heads,target_mask,num_layers)\n",
        "      self.output = OutputLayer(dmodel, tar_vocab_size)\n",
        "\n",
        "  def forward(self,src_token_ids,tar_token_ids):\n",
        "\n",
        "      out = self.encoder(self.src_pos_embeddings(self.src_embeddings(src_token_ids)))\n",
        "      out = self.decoder(out,self.tar_pos_embeddings(self.tar_embeddings(tar_token_ids)))\n",
        "      out = self.output(out)\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "yCug8q1GSIjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(src_vocab_size,tar_vocab_size,enc_ctxt_len,dec_ctxt_len,dmodel,dq,dk,dv,d_ff,heads,mask)"
      ],
      "metadata": {
        "id": "awmAIEf7kaMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj-pfuUphpUn",
        "outputId": "5391d514-de2a-489f-c2d7-2d4105a8fb5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (src_embeddings): Embedding(62, 32)\n",
              "  (tar_embeddings): Embedding(70, 32)\n",
              "  (src_pos_embeddings): PositionalEncoding()\n",
              "  (tar_pos_embeddings): PositionalEncoding()\n",
              "  (encoder): Encoder(\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (mha): MHA()\n",
              "        (layer_norm_mha): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (ffn): FFN()\n",
              "        (layer_norm_ffn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (decoder_layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): MHMA()\n",
              "        (cross_attention): MHCA()\n",
              "        (feed_forward): FFN()\n",
              "        (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (layer_norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (output): OutputLayer()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "A18BP8POk772"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train(src_token_ids, tar_token_ids, labels, model, criterion, optimizer, tar_vocab_size, dec_ctxt_len, batch_size, epochs=1000):\n",
        "    loss_trace = []\n",
        "    tf_epochs = int(epochs * 0.25)\n",
        "    ar_epochs = epochs - tf_epochs\n",
        "\n",
        "    print(\"Starting Training...\")\n",
        "\n",
        "    print(\"Teacher Forcing\")\n",
        "    for epoch in range(tf_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src_token_ids, tar_token_ids)\n",
        "        outputs = outputs.view(-1, tar_vocab_size)\n",
        "        target = labels.view(-1)\n",
        "        loss = criterion(outputs, target.long())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            loss_trace.append(loss.item())\n",
        "            print(f\"Epoch {epoch + 1}/{tf_epochs} | TF Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    print(\"Auto Regressive\")\n",
        "    for epoch in range(ar_epochs):\n",
        "        tar_token_ids = torch.zeros((batch_size, dec_ctxt_len), dtype=torch.int).to(src_token_ids.device)\n",
        "        tar_token_ids[:, 0] = 1\n",
        "\n",
        "        for step in range(dec_ctxt_len - 1):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(src_token_ids, tar_token_ids)\n",
        "            outputs_flat = outputs.view(-1, tar_vocab_size)\n",
        "            target_flat = labels.view(-1)\n",
        "\n",
        "            loss = criterion(outputs_flat, target_flat.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                predictions = torch.argmax(F.softmax(outputs, dim=-1), dim=-1)\n",
        "                tar_token_ids[:, step + 1] = predictions[:, step]\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                loss_trace.append(loss.item())\n",
        "                print(f\"Epoch {epoch + 1}/{ar_epochs} | AR Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "        loss_trace.append(loss.item())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(loss_trace, label='Loss')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Iterations')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Training completed. Total iterations: {len(loss_trace)}\")\n",
        "\n",
        "    return outputs, loss_trace"
      ],
      "metadata": {
        "id": "yWbFM5HnlXSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out, loss_trace = train(x,y,label,model,criterion, optimizer, tar_vocab_size, dec_ctxt_len, batch_size, epochs=1000)"
      ],
      "metadata": {
        "id": "HyslfJ1KZ51L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6876126f-8461-4382-e44a-198beb299fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n",
            "Teacher Forcing\n",
            "Epoch 10/250 | TF Loss: 9.8029\n",
            "Epoch 20/250 | TF Loss: 7.8297\n",
            "Epoch 30/250 | TF Loss: 6.7456\n",
            "Epoch 40/250 | TF Loss: 6.0258\n",
            "Epoch 50/250 | TF Loss: 5.4432\n",
            "Epoch 60/250 | TF Loss: 5.0041\n",
            "Epoch 70/250 | TF Loss: 4.6547\n",
            "Epoch 80/250 | TF Loss: 4.3708\n",
            "Epoch 90/250 | TF Loss: 4.1358\n",
            "Epoch 100/250 | TF Loss: 3.9417\n",
            "Epoch 110/250 | TF Loss: 3.7701\n",
            "Epoch 120/250 | TF Loss: 3.6146\n",
            "Epoch 130/250 | TF Loss: 3.4775\n",
            "Epoch 140/250 | TF Loss: 3.3524\n",
            "Epoch 150/250 | TF Loss: 3.2345\n",
            "Epoch 160/250 | TF Loss: 3.1230\n",
            "Epoch 170/250 | TF Loss: 3.0169\n",
            "Epoch 180/250 | TF Loss: 2.9165\n",
            "Epoch 190/250 | TF Loss: 2.8194\n",
            "Epoch 200/250 | TF Loss: 2.7257\n",
            "Epoch 210/250 | TF Loss: 2.6373\n",
            "Epoch 220/250 | TF Loss: 2.5508\n",
            "Epoch 230/250 | TF Loss: 2.4685\n",
            "Epoch 240/250 | TF Loss: 2.3886\n",
            "Epoch 250/250 | TF Loss: 2.3122\n",
            "Auto Regressive\n",
            "Epoch 10/750 | AR Loss: 3.2212\n",
            "Epoch 10/750 | AR Loss: 3.1530\n",
            "Epoch 10/750 | AR Loss: 3.0584\n",
            "Epoch 10/750 | AR Loss: 3.0067\n",
            "Epoch 10/750 | AR Loss: 3.0261\n",
            "Epoch 10/750 | AR Loss: 3.0740\n",
            "Epoch 10/750 | AR Loss: 3.0546\n",
            "Epoch 10/750 | AR Loss: 3.0796\n",
            "Epoch 10/750 | AR Loss: 3.1312\n",
            "Epoch 10/750 | AR Loss: 3.1604\n",
            "Epoch 10/750 | AR Loss: 3.1455\n",
            "Epoch 10/750 | AR Loss: 3.1342\n",
            "Epoch 20/750 | AR Loss: 3.0616\n",
            "Epoch 20/750 | AR Loss: 2.8174\n",
            "Epoch 20/750 | AR Loss: 2.7366\n",
            "Epoch 20/750 | AR Loss: 2.6689\n",
            "Epoch 20/750 | AR Loss: 2.6882\n",
            "Epoch 20/750 | AR Loss: 2.7367\n",
            "Epoch 20/750 | AR Loss: 2.7281\n",
            "Epoch 20/750 | AR Loss: 2.7299\n",
            "Epoch 20/750 | AR Loss: 2.7915\n",
            "Epoch 20/750 | AR Loss: 2.7658\n",
            "Epoch 20/750 | AR Loss: 2.7612\n",
            "Epoch 20/750 | AR Loss: 2.7408\n",
            "Epoch 30/750 | AR Loss: 2.8897\n",
            "Epoch 30/750 | AR Loss: 2.5560\n",
            "Epoch 30/750 | AR Loss: 2.4528\n",
            "Epoch 30/750 | AR Loss: 2.4116\n",
            "Epoch 30/750 | AR Loss: 2.3922\n",
            "Epoch 30/750 | AR Loss: 2.4105\n",
            "Epoch 30/750 | AR Loss: 2.4009\n",
            "Epoch 30/750 | AR Loss: 2.3929\n",
            "Epoch 30/750 | AR Loss: 2.4441\n",
            "Epoch 30/750 | AR Loss: 2.4908\n",
            "Epoch 30/750 | AR Loss: 2.5448\n",
            "Epoch 30/750 | AR Loss: 2.5584\n",
            "Epoch 40/750 | AR Loss: 2.6622\n",
            "Epoch 40/750 | AR Loss: 2.3483\n",
            "Epoch 40/750 | AR Loss: 2.2220\n",
            "Epoch 40/750 | AR Loss: 2.1673\n",
            "Epoch 40/750 | AR Loss: 2.1383\n",
            "Epoch 40/750 | AR Loss: 2.1326\n",
            "Epoch 40/750 | AR Loss: 2.1469\n",
            "Epoch 40/750 | AR Loss: 2.2076\n",
            "Epoch 40/750 | AR Loss: 2.3001\n",
            "Epoch 40/750 | AR Loss: 2.3797\n",
            "Epoch 40/750 | AR Loss: 2.4451\n",
            "Epoch 40/750 | AR Loss: 2.4696\n",
            "Epoch 50/750 | AR Loss: 2.4875\n",
            "Epoch 50/750 | AR Loss: 2.1841\n",
            "Epoch 50/750 | AR Loss: 2.0384\n",
            "Epoch 50/750 | AR Loss: 1.9516\n",
            "Epoch 50/750 | AR Loss: 1.9071\n",
            "Epoch 50/750 | AR Loss: 1.8851\n",
            "Epoch 50/750 | AR Loss: 1.8198\n",
            "Epoch 50/750 | AR Loss: 1.8905\n",
            "Epoch 50/750 | AR Loss: 1.9623\n",
            "Epoch 50/750 | AR Loss: 1.9956\n",
            "Epoch 50/750 | AR Loss: 2.0504\n",
            "Epoch 50/750 | AR Loss: 2.0747\n",
            "Epoch 60/750 | AR Loss: 2.2986\n",
            "Epoch 60/750 | AR Loss: 1.9484\n",
            "Epoch 60/750 | AR Loss: 1.7875\n",
            "Epoch 60/750 | AR Loss: 1.7350\n",
            "Epoch 60/750 | AR Loss: 1.6618\n",
            "Epoch 60/750 | AR Loss: 1.6655\n",
            "Epoch 60/750 | AR Loss: 1.6201\n",
            "Epoch 60/750 | AR Loss: 1.6531\n",
            "Epoch 60/750 | AR Loss: 1.6952\n",
            "Epoch 60/750 | AR Loss: 1.7212\n",
            "Epoch 60/750 | AR Loss: 1.7731\n",
            "Epoch 60/750 | AR Loss: 1.8367\n",
            "Epoch 70/750 | AR Loss: 2.1278\n",
            "Epoch 70/750 | AR Loss: 1.7613\n",
            "Epoch 70/750 | AR Loss: 1.6059\n",
            "Epoch 70/750 | AR Loss: 1.5160\n",
            "Epoch 70/750 | AR Loss: 1.4238\n",
            "Epoch 70/750 | AR Loss: 1.3785\n",
            "Epoch 70/750 | AR Loss: 1.3079\n",
            "Epoch 70/750 | AR Loss: 1.3687\n",
            "Epoch 70/750 | AR Loss: 1.4131\n",
            "Epoch 70/750 | AR Loss: 1.4717\n",
            "Epoch 70/750 | AR Loss: 1.5124\n",
            "Epoch 70/750 | AR Loss: 1.6256\n",
            "Epoch 80/750 | AR Loss: 1.9821\n",
            "Epoch 80/750 | AR Loss: 1.6507\n",
            "Epoch 80/750 | AR Loss: 1.4744\n",
            "Epoch 80/750 | AR Loss: 1.3976\n",
            "Epoch 80/750 | AR Loss: 1.3206\n",
            "Epoch 80/750 | AR Loss: 1.2603\n",
            "Epoch 80/750 | AR Loss: 1.2019\n",
            "Epoch 80/750 | AR Loss: 1.2351\n",
            "Epoch 80/750 | AR Loss: 1.2758\n",
            "Epoch 80/750 | AR Loss: 1.3252\n",
            "Epoch 80/750 | AR Loss: 1.3505\n",
            "Epoch 80/750 | AR Loss: 1.4109\n",
            "Epoch 90/750 | AR Loss: 1.8041\n",
            "Epoch 90/750 | AR Loss: 1.4581\n",
            "Epoch 90/750 | AR Loss: 1.2911\n",
            "Epoch 90/750 | AR Loss: 1.2235\n",
            "Epoch 90/750 | AR Loss: 1.1552\n",
            "Epoch 90/750 | AR Loss: 1.0876\n",
            "Epoch 90/750 | AR Loss: 1.1164\n",
            "Epoch 90/750 | AR Loss: 1.1109\n",
            "Epoch 90/750 | AR Loss: 1.0720\n",
            "Epoch 90/750 | AR Loss: 1.1148\n",
            "Epoch 90/750 | AR Loss: 1.1854\n",
            "Epoch 90/750 | AR Loss: 1.1962\n",
            "Epoch 100/750 | AR Loss: 1.6342\n",
            "Epoch 100/750 | AR Loss: 1.2775\n",
            "Epoch 100/750 | AR Loss: 1.1208\n",
            "Epoch 100/750 | AR Loss: 1.0583\n",
            "Epoch 100/750 | AR Loss: 1.0198\n",
            "Epoch 100/750 | AR Loss: 0.9321\n",
            "Epoch 100/750 | AR Loss: 0.9625\n",
            "Epoch 100/750 | AR Loss: 1.0027\n",
            "Epoch 100/750 | AR Loss: 0.9428\n",
            "Epoch 100/750 | AR Loss: 0.9494\n",
            "Epoch 100/750 | AR Loss: 1.0218\n",
            "Epoch 100/750 | AR Loss: 1.0087\n",
            "Epoch 110/750 | AR Loss: 1.5039\n",
            "Epoch 110/750 | AR Loss: 1.1531\n",
            "Epoch 110/750 | AR Loss: 1.0552\n",
            "Epoch 110/750 | AR Loss: 1.0013\n",
            "Epoch 110/750 | AR Loss: 0.9964\n",
            "Epoch 110/750 | AR Loss: 0.8966\n",
            "Epoch 110/750 | AR Loss: 0.8671\n",
            "Epoch 110/750 | AR Loss: 0.8166\n",
            "Epoch 110/750 | AR Loss: 0.8366\n",
            "Epoch 110/750 | AR Loss: 0.8980\n",
            "Epoch 110/750 | AR Loss: 0.9324\n",
            "Epoch 110/750 | AR Loss: 0.9541\n",
            "Epoch 120/750 | AR Loss: 1.3726\n",
            "Epoch 120/750 | AR Loss: 1.0355\n",
            "Epoch 120/750 | AR Loss: 0.9334\n",
            "Epoch 120/750 | AR Loss: 0.8894\n",
            "Epoch 120/750 | AR Loss: 0.8760\n",
            "Epoch 120/750 | AR Loss: 0.7830\n",
            "Epoch 120/750 | AR Loss: 0.8023\n",
            "Epoch 120/750 | AR Loss: 0.7598\n",
            "Epoch 120/750 | AR Loss: 0.6967\n",
            "Epoch 120/750 | AR Loss: 0.7821\n",
            "Epoch 120/750 | AR Loss: 0.8443\n",
            "Epoch 120/750 | AR Loss: 0.8489\n",
            "Epoch 130/750 | AR Loss: 1.2667\n",
            "Epoch 130/750 | AR Loss: 0.9154\n",
            "Epoch 130/750 | AR Loss: 0.7902\n",
            "Epoch 130/750 | AR Loss: 0.7470\n",
            "Epoch 130/750 | AR Loss: 0.6723\n",
            "Epoch 130/750 | AR Loss: 0.5733\n",
            "Epoch 130/750 | AR Loss: 0.5650\n",
            "Epoch 130/750 | AR Loss: 0.5332\n",
            "Epoch 130/750 | AR Loss: 0.5369\n",
            "Epoch 130/750 | AR Loss: 0.6045\n",
            "Epoch 130/750 | AR Loss: 0.6184\n",
            "Epoch 130/750 | AR Loss: 0.6604\n",
            "Epoch 140/750 | AR Loss: 1.1545\n",
            "Epoch 140/750 | AR Loss: 0.8165\n",
            "Epoch 140/750 | AR Loss: 0.6882\n",
            "Epoch 140/750 | AR Loss: 0.6694\n",
            "Epoch 140/750 | AR Loss: 0.5983\n",
            "Epoch 140/750 | AR Loss: 0.5241\n",
            "Epoch 140/750 | AR Loss: 0.4868\n",
            "Epoch 140/750 | AR Loss: 0.4269\n",
            "Epoch 140/750 | AR Loss: 0.4129\n",
            "Epoch 140/750 | AR Loss: 0.4465\n",
            "Epoch 140/750 | AR Loss: 0.4539\n",
            "Epoch 140/750 | AR Loss: 0.4716\n",
            "Epoch 150/750 | AR Loss: 1.0396\n",
            "Epoch 150/750 | AR Loss: 0.7374\n",
            "Epoch 150/750 | AR Loss: 0.6241\n",
            "Epoch 150/750 | AR Loss: 0.5830\n",
            "Epoch 150/750 | AR Loss: 0.5250\n",
            "Epoch 150/750 | AR Loss: 0.4325\n",
            "Epoch 150/750 | AR Loss: 0.4073\n",
            "Epoch 150/750 | AR Loss: 0.3872\n",
            "Epoch 150/750 | AR Loss: 0.4148\n",
            "Epoch 150/750 | AR Loss: 0.4761\n",
            "Epoch 150/750 | AR Loss: 0.4905\n",
            "Epoch 150/750 | AR Loss: 0.5700\n",
            "Epoch 160/750 | AR Loss: 0.9134\n",
            "Epoch 160/750 | AR Loss: 0.6598\n",
            "Epoch 160/750 | AR Loss: 0.5685\n",
            "Epoch 160/750 | AR Loss: 0.5190\n",
            "Epoch 160/750 | AR Loss: 0.4777\n",
            "Epoch 160/750 | AR Loss: 0.3813\n",
            "Epoch 160/750 | AR Loss: 0.3558\n",
            "Epoch 160/750 | AR Loss: 0.3107\n",
            "Epoch 160/750 | AR Loss: 0.2946\n",
            "Epoch 160/750 | AR Loss: 0.3075\n",
            "Epoch 160/750 | AR Loss: 0.3253\n",
            "Epoch 160/750 | AR Loss: 0.3499\n",
            "Epoch 170/750 | AR Loss: 0.7953\n",
            "Epoch 170/750 | AR Loss: 0.5757\n",
            "Epoch 170/750 | AR Loss: 0.4969\n",
            "Epoch 170/750 | AR Loss: 0.4681\n",
            "Epoch 170/750 | AR Loss: 0.4234\n",
            "Epoch 170/750 | AR Loss: 0.3584\n",
            "Epoch 170/750 | AR Loss: 0.3253\n",
            "Epoch 170/750 | AR Loss: 0.2799\n",
            "Epoch 170/750 | AR Loss: 0.2702\n",
            "Epoch 170/750 | AR Loss: 0.2494\n",
            "Epoch 170/750 | AR Loss: 0.2700\n",
            "Epoch 170/750 | AR Loss: 0.3159\n",
            "Epoch 180/750 | AR Loss: 0.7437\n",
            "Epoch 180/750 | AR Loss: 0.5432\n",
            "Epoch 180/750 | AR Loss: 0.5083\n",
            "Epoch 180/750 | AR Loss: 0.6024\n",
            "Epoch 180/750 | AR Loss: 0.5863\n",
            "Epoch 180/750 | AR Loss: 0.7195\n",
            "Epoch 180/750 | AR Loss: 0.3563\n",
            "Epoch 180/750 | AR Loss: 0.2635\n",
            "Epoch 180/750 | AR Loss: 0.2186\n",
            "Epoch 180/750 | AR Loss: 0.2782\n",
            "Epoch 180/750 | AR Loss: 0.3310\n",
            "Epoch 180/750 | AR Loss: 0.3554\n",
            "Epoch 190/750 | AR Loss: 0.6555\n",
            "Epoch 190/750 | AR Loss: 0.4533\n",
            "Epoch 190/750 | AR Loss: 0.3931\n",
            "Epoch 190/750 | AR Loss: 0.3759\n",
            "Epoch 190/750 | AR Loss: 0.3390\n",
            "Epoch 190/750 | AR Loss: 0.2898\n",
            "Epoch 190/750 | AR Loss: 0.2389\n",
            "Epoch 190/750 | AR Loss: 0.1888\n",
            "Epoch 190/750 | AR Loss: 0.1644\n",
            "Epoch 190/750 | AR Loss: 0.1883\n",
            "Epoch 190/750 | AR Loss: 0.1737\n",
            "Epoch 190/750 | AR Loss: 0.1992\n",
            "Epoch 200/750 | AR Loss: 0.5735\n",
            "Epoch 200/750 | AR Loss: 0.4046\n",
            "Epoch 200/750 | AR Loss: 0.3537\n",
            "Epoch 200/750 | AR Loss: 0.3484\n",
            "Epoch 200/750 | AR Loss: 0.3185\n",
            "Epoch 200/750 | AR Loss: 0.2965\n",
            "Epoch 200/750 | AR Loss: 0.2416\n",
            "Epoch 200/750 | AR Loss: 0.1786\n",
            "Epoch 200/750 | AR Loss: 0.1455\n",
            "Epoch 200/750 | AR Loss: 0.1406\n",
            "Epoch 200/750 | AR Loss: 0.1234\n",
            "Epoch 200/750 | AR Loss: 0.1782\n",
            "Epoch 210/750 | AR Loss: 0.5045\n",
            "Epoch 210/750 | AR Loss: 0.3602\n",
            "Epoch 210/750 | AR Loss: 0.3142\n",
            "Epoch 210/750 | AR Loss: 0.3106\n",
            "Epoch 210/750 | AR Loss: 0.2863\n",
            "Epoch 210/750 | AR Loss: 0.2808\n",
            "Epoch 210/750 | AR Loss: 0.2468\n",
            "Epoch 210/750 | AR Loss: 0.2093\n",
            "Epoch 210/750 | AR Loss: 0.1582\n",
            "Epoch 210/750 | AR Loss: 0.1251\n",
            "Epoch 210/750 | AR Loss: 0.1019\n",
            "Epoch 210/750 | AR Loss: 0.1483\n",
            "Epoch 220/750 | AR Loss: 0.4520\n",
            "Epoch 220/750 | AR Loss: 0.3267\n",
            "Epoch 220/750 | AR Loss: 0.2908\n",
            "Epoch 220/750 | AR Loss: 0.2948\n",
            "Epoch 220/750 | AR Loss: 0.2758\n",
            "Epoch 220/750 | AR Loss: 0.3070\n",
            "Epoch 220/750 | AR Loss: 0.3190\n",
            "Epoch 220/750 | AR Loss: 0.3103\n",
            "Epoch 220/750 | AR Loss: 0.1348\n",
            "Epoch 220/750 | AR Loss: 0.1041\n",
            "Epoch 220/750 | AR Loss: 0.0872\n",
            "Epoch 220/750 | AR Loss: 0.1210\n",
            "Epoch 230/750 | AR Loss: 0.4027\n",
            "Epoch 230/750 | AR Loss: 0.2871\n",
            "Epoch 230/750 | AR Loss: 0.2538\n",
            "Epoch 230/750 | AR Loss: 0.2334\n",
            "Epoch 230/750 | AR Loss: 0.2110\n",
            "Epoch 230/750 | AR Loss: 0.1797\n",
            "Epoch 230/750 | AR Loss: 0.1517\n",
            "Epoch 230/750 | AR Loss: 0.1196\n",
            "Epoch 230/750 | AR Loss: 0.0994\n",
            "Epoch 230/750 | AR Loss: 0.0982\n",
            "Epoch 230/750 | AR Loss: 0.0672\n",
            "Epoch 230/750 | AR Loss: 0.1007\n",
            "Epoch 240/750 | AR Loss: 0.3591\n",
            "Epoch 240/750 | AR Loss: 0.2562\n",
            "Epoch 240/750 | AR Loss: 0.2282\n",
            "Epoch 240/750 | AR Loss: 0.2005\n",
            "Epoch 240/750 | AR Loss: 0.1879\n",
            "Epoch 240/750 | AR Loss: 0.1585\n",
            "Epoch 240/750 | AR Loss: 0.1423\n",
            "Epoch 240/750 | AR Loss: 0.1189\n",
            "Epoch 240/750 | AR Loss: 0.1035\n",
            "Epoch 240/750 | AR Loss: 0.0934\n",
            "Epoch 240/750 | AR Loss: 0.0573\n",
            "Epoch 240/750 | AR Loss: 0.0812\n",
            "Epoch 250/750 | AR Loss: 0.3618\n",
            "Epoch 250/750 | AR Loss: 0.3101\n",
            "Epoch 250/750 | AR Loss: 0.2719\n",
            "Epoch 250/750 | AR Loss: 0.2304\n",
            "Epoch 250/750 | AR Loss: 0.2868\n",
            "Epoch 250/750 | AR Loss: 0.3161\n",
            "Epoch 250/750 | AR Loss: 0.5100\n",
            "Epoch 250/750 | AR Loss: 0.2549\n",
            "Epoch 250/750 | AR Loss: 0.1355\n",
            "Epoch 250/750 | AR Loss: 0.0935\n",
            "Epoch 250/750 | AR Loss: 0.0574\n",
            "Epoch 250/750 | AR Loss: 0.0677\n",
            "Epoch 260/750 | AR Loss: 0.2910\n",
            "Epoch 260/750 | AR Loss: 0.2106\n",
            "Epoch 260/750 | AR Loss: 0.1881\n",
            "Epoch 260/750 | AR Loss: 0.1645\n",
            "Epoch 260/750 | AR Loss: 0.1567\n",
            "Epoch 260/750 | AR Loss: 0.1346\n",
            "Epoch 260/750 | AR Loss: 0.1162\n",
            "Epoch 260/750 | AR Loss: 0.1004\n",
            "Epoch 260/750 | AR Loss: 0.0782\n",
            "Epoch 260/750 | AR Loss: 0.0809\n",
            "Epoch 260/750 | AR Loss: 0.0477\n",
            "Epoch 260/750 | AR Loss: 0.0549\n",
            "Epoch 270/750 | AR Loss: 0.2609\n",
            "Epoch 270/750 | AR Loss: 0.1948\n",
            "Epoch 270/750 | AR Loss: 0.1707\n",
            "Epoch 270/750 | AR Loss: 0.1450\n",
            "Epoch 270/750 | AR Loss: 0.1412\n",
            "Epoch 270/750 | AR Loss: 0.1190\n",
            "Epoch 270/750 | AR Loss: 0.1111\n",
            "Epoch 270/750 | AR Loss: 0.0998\n",
            "Epoch 270/750 | AR Loss: 0.0835\n",
            "Epoch 270/750 | AR Loss: 0.0811\n",
            "Epoch 270/750 | AR Loss: 0.0411\n",
            "Epoch 270/750 | AR Loss: 0.0487\n",
            "Epoch 280/750 | AR Loss: 0.2353\n",
            "Epoch 280/750 | AR Loss: 0.1808\n",
            "Epoch 280/750 | AR Loss: 0.1575\n",
            "Epoch 280/750 | AR Loss: 0.1317\n",
            "Epoch 280/750 | AR Loss: 0.1288\n",
            "Epoch 280/750 | AR Loss: 0.1091\n",
            "Epoch 280/750 | AR Loss: 0.1012\n",
            "Epoch 280/750 | AR Loss: 0.0922\n",
            "Epoch 280/750 | AR Loss: 0.0791\n",
            "Epoch 280/750 | AR Loss: 0.0779\n",
            "Epoch 280/750 | AR Loss: 0.0367\n",
            "Epoch 280/750 | AR Loss: 0.0441\n",
            "Epoch 290/750 | AR Loss: 0.2147\n",
            "Epoch 290/750 | AR Loss: 0.1708\n",
            "Epoch 290/750 | AR Loss: 0.1466\n",
            "Epoch 290/750 | AR Loss: 0.1203\n",
            "Epoch 290/750 | AR Loss: 0.1198\n",
            "Epoch 290/750 | AR Loss: 0.0981\n",
            "Epoch 290/750 | AR Loss: 0.0992\n",
            "Epoch 290/750 | AR Loss: 0.0991\n",
            "Epoch 290/750 | AR Loss: 0.0980\n",
            "Epoch 290/750 | AR Loss: 0.0829\n",
            "Epoch 290/750 | AR Loss: 0.0348\n",
            "Epoch 290/750 | AR Loss: 0.0400\n",
            "Epoch 300/750 | AR Loss: 0.1968\n",
            "Epoch 300/750 | AR Loss: 0.1565\n",
            "Epoch 300/750 | AR Loss: 0.1346\n",
            "Epoch 300/750 | AR Loss: 0.1107\n",
            "Epoch 300/750 | AR Loss: 0.1101\n",
            "Epoch 300/750 | AR Loss: 0.0923\n",
            "Epoch 300/750 | AR Loss: 0.0865\n",
            "Epoch 300/750 | AR Loss: 0.0813\n",
            "Epoch 300/750 | AR Loss: 0.0627\n",
            "Epoch 300/750 | AR Loss: 0.0615\n",
            "Epoch 300/750 | AR Loss: 0.0327\n",
            "Epoch 300/750 | AR Loss: 0.0373\n",
            "Epoch 310/750 | AR Loss: 0.1804\n",
            "Epoch 310/750 | AR Loss: 0.1437\n",
            "Epoch 310/750 | AR Loss: 0.1217\n",
            "Epoch 310/750 | AR Loss: 0.1000\n",
            "Epoch 310/750 | AR Loss: 0.1004\n",
            "Epoch 310/750 | AR Loss: 0.0836\n",
            "Epoch 310/750 | AR Loss: 0.0823\n",
            "Epoch 310/750 | AR Loss: 0.0856\n",
            "Epoch 310/750 | AR Loss: 0.0773\n",
            "Epoch 310/750 | AR Loss: 0.0671\n",
            "Epoch 310/750 | AR Loss: 0.0289\n",
            "Epoch 310/750 | AR Loss: 0.0342\n",
            "Epoch 320/750 | AR Loss: 0.1650\n",
            "Epoch 320/750 | AR Loss: 0.1336\n",
            "Epoch 320/750 | AR Loss: 0.1137\n",
            "Epoch 320/750 | AR Loss: 0.0921\n",
            "Epoch 320/750 | AR Loss: 0.0921\n",
            "Epoch 320/750 | AR Loss: 0.0768\n",
            "Epoch 320/750 | AR Loss: 0.0747\n",
            "Epoch 320/750 | AR Loss: 0.0750\n",
            "Epoch 320/750 | AR Loss: 0.0580\n",
            "Epoch 320/750 | AR Loss: 0.0533\n",
            "Epoch 320/750 | AR Loss: 0.0271\n",
            "Epoch 320/750 | AR Loss: 0.0320\n",
            "Epoch 330/750 | AR Loss: 0.1518\n",
            "Epoch 330/750 | AR Loss: 0.1210\n",
            "Epoch 330/750 | AR Loss: 0.1019\n",
            "Epoch 330/750 | AR Loss: 0.0832\n",
            "Epoch 330/750 | AR Loss: 0.0837\n",
            "Epoch 330/750 | AR Loss: 0.0699\n",
            "Epoch 330/750 | AR Loss: 0.0707\n",
            "Epoch 330/750 | AR Loss: 0.0724\n",
            "Epoch 330/750 | AR Loss: 0.0577\n",
            "Epoch 330/750 | AR Loss: 0.0530\n",
            "Epoch 330/750 | AR Loss: 0.0243\n",
            "Epoch 330/750 | AR Loss: 0.0295\n",
            "Epoch 340/750 | AR Loss: 0.1403\n",
            "Epoch 340/750 | AR Loss: 0.1125\n",
            "Epoch 340/750 | AR Loss: 0.0937\n",
            "Epoch 340/750 | AR Loss: 0.0763\n",
            "Epoch 340/750 | AR Loss: 0.0773\n",
            "Epoch 340/750 | AR Loss: 0.0642\n",
            "Epoch 340/750 | AR Loss: 0.0660\n",
            "Epoch 340/750 | AR Loss: 0.0681\n",
            "Epoch 340/750 | AR Loss: 0.0543\n",
            "Epoch 340/750 | AR Loss: 0.0489\n",
            "Epoch 340/750 | AR Loss: 0.0229\n",
            "Epoch 340/750 | AR Loss: 0.0276\n",
            "Epoch 350/750 | AR Loss: 0.1289\n",
            "Epoch 350/750 | AR Loss: 0.1022\n",
            "Epoch 350/750 | AR Loss: 0.0857\n",
            "Epoch 350/750 | AR Loss: 0.0709\n",
            "Epoch 350/750 | AR Loss: 0.0713\n",
            "Epoch 350/750 | AR Loss: 0.0596\n",
            "Epoch 350/750 | AR Loss: 0.0601\n",
            "Epoch 350/750 | AR Loss: 0.0620\n",
            "Epoch 350/750 | AR Loss: 0.0463\n",
            "Epoch 350/750 | AR Loss: 0.0447\n",
            "Epoch 350/750 | AR Loss: 0.0217\n",
            "Epoch 350/750 | AR Loss: 0.0255\n",
            "Epoch 360/750 | AR Loss: 0.1192\n",
            "Epoch 360/750 | AR Loss: 0.0951\n",
            "Epoch 360/750 | AR Loss: 0.0792\n",
            "Epoch 360/750 | AR Loss: 0.0653\n",
            "Epoch 360/750 | AR Loss: 0.0656\n",
            "Epoch 360/750 | AR Loss: 0.0550\n",
            "Epoch 360/750 | AR Loss: 0.0578\n",
            "Epoch 360/750 | AR Loss: 0.0599\n",
            "Epoch 360/750 | AR Loss: 0.0470\n",
            "Epoch 360/750 | AR Loss: 0.0436\n",
            "Epoch 360/750 | AR Loss: 0.0203\n",
            "Epoch 360/750 | AR Loss: 0.0236\n",
            "Epoch 370/750 | AR Loss: 0.1109\n",
            "Epoch 370/750 | AR Loss: 0.0899\n",
            "Epoch 370/750 | AR Loss: 0.0744\n",
            "Epoch 370/750 | AR Loss: 0.0601\n",
            "Epoch 370/750 | AR Loss: 0.0609\n",
            "Epoch 370/750 | AR Loss: 0.0507\n",
            "Epoch 370/750 | AR Loss: 0.0570\n",
            "Epoch 370/750 | AR Loss: 0.0617\n",
            "Epoch 370/750 | AR Loss: 0.0569\n",
            "Epoch 370/750 | AR Loss: 0.0466\n",
            "Epoch 370/750 | AR Loss: 0.0191\n",
            "Epoch 370/750 | AR Loss: 0.0219\n",
            "Epoch 380/750 | AR Loss: 0.1042\n",
            "Epoch 380/750 | AR Loss: 0.0855\n",
            "Epoch 380/750 | AR Loss: 0.0703\n",
            "Epoch 380/750 | AR Loss: 0.0561\n",
            "Epoch 380/750 | AR Loss: 0.0567\n",
            "Epoch 380/750 | AR Loss: 0.0472\n",
            "Epoch 380/750 | AR Loss: 0.0533\n",
            "Epoch 380/750 | AR Loss: 0.0549\n",
            "Epoch 380/750 | AR Loss: 0.0439\n",
            "Epoch 380/750 | AR Loss: 0.0394\n",
            "Epoch 380/750 | AR Loss: 0.0182\n",
            "Epoch 380/750 | AR Loss: 0.0208\n",
            "Epoch 390/750 | AR Loss: 0.0971\n",
            "Epoch 390/750 | AR Loss: 0.0800\n",
            "Epoch 390/750 | AR Loss: 0.0658\n",
            "Epoch 390/750 | AR Loss: 0.0526\n",
            "Epoch 390/750 | AR Loss: 0.0531\n",
            "Epoch 390/750 | AR Loss: 0.0444\n",
            "Epoch 390/750 | AR Loss: 0.0525\n",
            "Epoch 390/750 | AR Loss: 0.0568\n",
            "Epoch 390/750 | AR Loss: 0.0535\n",
            "Epoch 390/750 | AR Loss: 0.0443\n",
            "Epoch 390/750 | AR Loss: 0.0173\n",
            "Epoch 390/750 | AR Loss: 0.0193\n",
            "Epoch 400/750 | AR Loss: 0.0915\n",
            "Epoch 400/750 | AR Loss: 0.0762\n",
            "Epoch 400/750 | AR Loss: 0.0623\n",
            "Epoch 400/750 | AR Loss: 0.0491\n",
            "Epoch 400/750 | AR Loss: 0.0494\n",
            "Epoch 400/750 | AR Loss: 0.0416\n",
            "Epoch 400/750 | AR Loss: 0.0485\n",
            "Epoch 400/750 | AR Loss: 0.0522\n",
            "Epoch 400/750 | AR Loss: 0.0442\n",
            "Epoch 400/750 | AR Loss: 0.0368\n",
            "Epoch 400/750 | AR Loss: 0.0163\n",
            "Epoch 400/750 | AR Loss: 0.0183\n",
            "Epoch 410/750 | AR Loss: 0.0863\n",
            "Epoch 410/750 | AR Loss: 0.0722\n",
            "Epoch 410/750 | AR Loss: 0.0590\n",
            "Epoch 410/750 | AR Loss: 0.0465\n",
            "Epoch 410/750 | AR Loss: 0.0463\n",
            "Epoch 410/750 | AR Loss: 0.0392\n",
            "Epoch 410/750 | AR Loss: 0.0478\n",
            "Epoch 410/750 | AR Loss: 0.0549\n",
            "Epoch 410/750 | AR Loss: 0.0546\n",
            "Epoch 410/750 | AR Loss: 0.0439\n",
            "Epoch 410/750 | AR Loss: 0.0161\n",
            "Epoch 410/750 | AR Loss: 0.0172\n",
            "Epoch 420/750 | AR Loss: 0.0797\n",
            "Epoch 420/750 | AR Loss: 0.0651\n",
            "Epoch 420/750 | AR Loss: 0.0545\n",
            "Epoch 420/750 | AR Loss: 0.0427\n",
            "Epoch 420/750 | AR Loss: 0.0427\n",
            "Epoch 420/750 | AR Loss: 0.0367\n",
            "Epoch 420/750 | AR Loss: 0.0420\n",
            "Epoch 420/750 | AR Loss: 0.0448\n",
            "Epoch 420/750 | AR Loss: 0.0341\n",
            "Epoch 420/750 | AR Loss: 0.0312\n",
            "Epoch 420/750 | AR Loss: 0.0149\n",
            "Epoch 420/750 | AR Loss: 0.0160\n",
            "Epoch 430/750 | AR Loss: 0.0756\n",
            "Epoch 430/750 | AR Loss: 0.0625\n",
            "Epoch 430/750 | AR Loss: 0.0517\n",
            "Epoch 430/750 | AR Loss: 0.0402\n",
            "Epoch 430/750 | AR Loss: 0.0400\n",
            "Epoch 430/750 | AR Loss: 0.0346\n",
            "Epoch 430/750 | AR Loss: 0.0412\n",
            "Epoch 430/750 | AR Loss: 0.0436\n",
            "Epoch 430/750 | AR Loss: 0.0337\n",
            "Epoch 430/750 | AR Loss: 0.0301\n",
            "Epoch 430/750 | AR Loss: 0.0142\n",
            "Epoch 430/750 | AR Loss: 0.0150\n",
            "Epoch 440/750 | AR Loss: 0.0711\n",
            "Epoch 440/750 | AR Loss: 0.0586\n",
            "Epoch 440/750 | AR Loss: 0.0486\n",
            "Epoch 440/750 | AR Loss: 0.0381\n",
            "Epoch 440/750 | AR Loss: 0.0375\n",
            "Epoch 440/750 | AR Loss: 0.0327\n",
            "Epoch 440/750 | AR Loss: 0.0398\n",
            "Epoch 440/750 | AR Loss: 0.0426\n",
            "Epoch 440/750 | AR Loss: 0.0334\n",
            "Epoch 440/750 | AR Loss: 0.0295\n",
            "Epoch 440/750 | AR Loss: 0.0135\n",
            "Epoch 440/750 | AR Loss: 0.0143\n",
            "Epoch 450/750 | AR Loss: 0.0672\n",
            "Epoch 450/750 | AR Loss: 0.0558\n",
            "Epoch 450/750 | AR Loss: 0.0459\n",
            "Epoch 450/750 | AR Loss: 0.0358\n",
            "Epoch 450/750 | AR Loss: 0.0353\n",
            "Epoch 450/750 | AR Loss: 0.0310\n",
            "Epoch 450/750 | AR Loss: 0.0374\n",
            "Epoch 450/750 | AR Loss: 0.0395\n",
            "Epoch 450/750 | AR Loss: 0.0288\n",
            "Epoch 450/750 | AR Loss: 0.0264\n",
            "Epoch 450/750 | AR Loss: 0.0131\n",
            "Epoch 450/750 | AR Loss: 0.0137\n",
            "Epoch 460/750 | AR Loss: 0.0640\n",
            "Epoch 460/750 | AR Loss: 0.0530\n",
            "Epoch 460/750 | AR Loss: 0.0432\n",
            "Epoch 460/750 | AR Loss: 0.0339\n",
            "Epoch 460/750 | AR Loss: 0.0334\n",
            "Epoch 460/750 | AR Loss: 0.0294\n",
            "Epoch 460/750 | AR Loss: 0.0359\n",
            "Epoch 460/750 | AR Loss: 0.0379\n",
            "Epoch 460/750 | AR Loss: 0.0276\n",
            "Epoch 460/750 | AR Loss: 0.0249\n",
            "Epoch 460/750 | AR Loss: 0.0126\n",
            "Epoch 460/750 | AR Loss: 0.0130\n",
            "Epoch 470/750 | AR Loss: 0.0609\n",
            "Epoch 470/750 | AR Loss: 0.0501\n",
            "Epoch 470/750 | AR Loss: 0.0408\n",
            "Epoch 470/750 | AR Loss: 0.0322\n",
            "Epoch 470/750 | AR Loss: 0.0317\n",
            "Epoch 470/750 | AR Loss: 0.0278\n",
            "Epoch 470/750 | AR Loss: 0.0341\n",
            "Epoch 470/750 | AR Loss: 0.0361\n",
            "Epoch 470/750 | AR Loss: 0.0256\n",
            "Epoch 470/750 | AR Loss: 0.0234\n",
            "Epoch 470/750 | AR Loss: 0.0121\n",
            "Epoch 470/750 | AR Loss: 0.0124\n",
            "Epoch 480/750 | AR Loss: 0.0580\n",
            "Epoch 480/750 | AR Loss: 0.0480\n",
            "Epoch 480/750 | AR Loss: 0.0388\n",
            "Epoch 480/750 | AR Loss: 0.0306\n",
            "Epoch 480/750 | AR Loss: 0.0301\n",
            "Epoch 480/750 | AR Loss: 0.0265\n",
            "Epoch 480/750 | AR Loss: 0.0327\n",
            "Epoch 480/750 | AR Loss: 0.0350\n",
            "Epoch 480/750 | AR Loss: 0.0242\n",
            "Epoch 480/750 | AR Loss: 0.0220\n",
            "Epoch 480/750 | AR Loss: 0.0117\n",
            "Epoch 480/750 | AR Loss: 0.0118\n",
            "Epoch 490/750 | AR Loss: 0.0551\n",
            "Epoch 490/750 | AR Loss: 0.0452\n",
            "Epoch 490/750 | AR Loss: 0.0365\n",
            "Epoch 490/750 | AR Loss: 0.0290\n",
            "Epoch 490/750 | AR Loss: 0.0286\n",
            "Epoch 490/750 | AR Loss: 0.0251\n",
            "Epoch 490/750 | AR Loss: 0.0311\n",
            "Epoch 490/750 | AR Loss: 0.0328\n",
            "Epoch 490/750 | AR Loss: 0.0219\n",
            "Epoch 490/750 | AR Loss: 0.0209\n",
            "Epoch 490/750 | AR Loss: 0.0114\n",
            "Epoch 490/750 | AR Loss: 0.0113\n",
            "Epoch 500/750 | AR Loss: 0.0528\n",
            "Epoch 500/750 | AR Loss: 0.0436\n",
            "Epoch 500/750 | AR Loss: 0.0349\n",
            "Epoch 500/750 | AR Loss: 0.0280\n",
            "Epoch 500/750 | AR Loss: 0.0274\n",
            "Epoch 500/750 | AR Loss: 0.0241\n",
            "Epoch 500/750 | AR Loss: 0.0298\n",
            "Epoch 500/750 | AR Loss: 0.0322\n",
            "Epoch 500/750 | AR Loss: 0.0227\n",
            "Epoch 500/750 | AR Loss: 0.0206\n",
            "Epoch 500/750 | AR Loss: 0.0104\n",
            "Epoch 500/750 | AR Loss: 0.0108\n",
            "Epoch 510/750 | AR Loss: 0.0504\n",
            "Epoch 510/750 | AR Loss: 0.0413\n",
            "Epoch 510/750 | AR Loss: 0.0330\n",
            "Epoch 510/750 | AR Loss: 0.0262\n",
            "Epoch 510/750 | AR Loss: 0.0257\n",
            "Epoch 510/750 | AR Loss: 0.0227\n",
            "Epoch 510/750 | AR Loss: 0.0278\n",
            "Epoch 510/750 | AR Loss: 0.0298\n",
            "Epoch 510/750 | AR Loss: 0.0197\n",
            "Epoch 510/750 | AR Loss: 0.0189\n",
            "Epoch 510/750 | AR Loss: 0.0109\n",
            "Epoch 510/750 | AR Loss: 0.0103\n",
            "Epoch 520/750 | AR Loss: 0.0481\n",
            "Epoch 520/750 | AR Loss: 0.0399\n",
            "Epoch 520/750 | AR Loss: 0.0315\n",
            "Epoch 520/750 | AR Loss: 0.0252\n",
            "Epoch 520/750 | AR Loss: 0.0247\n",
            "Epoch 520/750 | AR Loss: 0.0218\n",
            "Epoch 520/750 | AR Loss: 0.0270\n",
            "Epoch 520/750 | AR Loss: 0.0290\n",
            "Epoch 520/750 | AR Loss: 0.0195\n",
            "Epoch 520/750 | AR Loss: 0.0181\n",
            "Epoch 520/750 | AR Loss: 0.0099\n",
            "Epoch 520/750 | AR Loss: 0.0100\n",
            "Epoch 530/750 | AR Loss: 0.0458\n",
            "Epoch 530/750 | AR Loss: 0.0378\n",
            "Epoch 530/750 | AR Loss: 0.0299\n",
            "Epoch 530/750 | AR Loss: 0.0239\n",
            "Epoch 530/750 | AR Loss: 0.0234\n",
            "Epoch 530/750 | AR Loss: 0.0207\n",
            "Epoch 530/750 | AR Loss: 0.0256\n",
            "Epoch 530/750 | AR Loss: 0.0276\n",
            "Epoch 530/750 | AR Loss: 0.0180\n",
            "Epoch 530/750 | AR Loss: 0.0174\n",
            "Epoch 530/750 | AR Loss: 0.0100\n",
            "Epoch 530/750 | AR Loss: 0.0096\n",
            "Epoch 540/750 | AR Loss: 0.0438\n",
            "Epoch 540/750 | AR Loss: 0.0359\n",
            "Epoch 540/750 | AR Loss: 0.0285\n",
            "Epoch 540/750 | AR Loss: 0.0228\n",
            "Epoch 540/750 | AR Loss: 0.0222\n",
            "Epoch 540/750 | AR Loss: 0.0197\n",
            "Epoch 540/750 | AR Loss: 0.0245\n",
            "Epoch 540/750 | AR Loss: 0.0266\n",
            "Epoch 540/750 | AR Loss: 0.0171\n",
            "Epoch 540/750 | AR Loss: 0.0166\n",
            "Epoch 540/750 | AR Loss: 0.0093\n",
            "Epoch 540/750 | AR Loss: 0.0092\n",
            "Epoch 550/750 | AR Loss: 0.0421\n",
            "Epoch 550/750 | AR Loss: 0.0346\n",
            "Epoch 550/750 | AR Loss: 0.0272\n",
            "Epoch 550/750 | AR Loss: 0.0218\n",
            "Epoch 550/750 | AR Loss: 0.0213\n",
            "Epoch 550/750 | AR Loss: 0.0189\n",
            "Epoch 550/750 | AR Loss: 0.0237\n",
            "Epoch 550/750 | AR Loss: 0.0259\n",
            "Epoch 550/750 | AR Loss: 0.0164\n",
            "Epoch 550/750 | AR Loss: 0.0159\n",
            "Epoch 550/750 | AR Loss: 0.0088\n",
            "Epoch 550/750 | AR Loss: 0.0089\n",
            "Epoch 560/750 | AR Loss: 0.0405\n",
            "Epoch 560/750 | AR Loss: 0.0329\n",
            "Epoch 560/750 | AR Loss: 0.0260\n",
            "Epoch 560/750 | AR Loss: 0.0209\n",
            "Epoch 560/750 | AR Loss: 0.0204\n",
            "Epoch 560/750 | AR Loss: 0.0181\n",
            "Epoch 560/750 | AR Loss: 0.0226\n",
            "Epoch 560/750 | AR Loss: 0.0244\n",
            "Epoch 560/750 | AR Loss: 0.0151\n",
            "Epoch 560/750 | AR Loss: 0.0154\n",
            "Epoch 560/750 | AR Loss: 0.0092\n",
            "Epoch 560/750 | AR Loss: 0.0085\n",
            "Epoch 570/750 | AR Loss: 0.0389\n",
            "Epoch 570/750 | AR Loss: 0.0315\n",
            "Epoch 570/750 | AR Loss: 0.0249\n",
            "Epoch 570/750 | AR Loss: 0.0200\n",
            "Epoch 570/750 | AR Loss: 0.0195\n",
            "Epoch 570/750 | AR Loss: 0.0174\n",
            "Epoch 570/750 | AR Loss: 0.0218\n",
            "Epoch 570/750 | AR Loss: 0.0235\n",
            "Epoch 570/750 | AR Loss: 0.0145\n",
            "Epoch 570/750 | AR Loss: 0.0148\n",
            "Epoch 570/750 | AR Loss: 0.0087\n",
            "Epoch 570/750 | AR Loss: 0.0083\n",
            "Epoch 580/750 | AR Loss: 0.0374\n",
            "Epoch 580/750 | AR Loss: 0.0302\n",
            "Epoch 580/750 | AR Loss: 0.0238\n",
            "Epoch 580/750 | AR Loss: 0.0192\n",
            "Epoch 580/750 | AR Loss: 0.0187\n",
            "Epoch 580/750 | AR Loss: 0.0167\n",
            "Epoch 580/750 | AR Loss: 0.0209\n",
            "Epoch 580/750 | AR Loss: 0.0228\n",
            "Epoch 580/750 | AR Loss: 0.0136\n",
            "Epoch 580/750 | AR Loss: 0.0143\n",
            "Epoch 580/750 | AR Loss: 0.0084\n",
            "Epoch 580/750 | AR Loss: 0.0079\n",
            "Epoch 590/750 | AR Loss: 0.0361\n",
            "Epoch 590/750 | AR Loss: 0.0290\n",
            "Epoch 590/750 | AR Loss: 0.0228\n",
            "Epoch 590/750 | AR Loss: 0.0185\n",
            "Epoch 590/750 | AR Loss: 0.0181\n",
            "Epoch 590/750 | AR Loss: 0.0160\n",
            "Epoch 590/750 | AR Loss: 0.0202\n",
            "Epoch 590/750 | AR Loss: 0.0220\n",
            "Epoch 590/750 | AR Loss: 0.0132\n",
            "Epoch 590/750 | AR Loss: 0.0135\n",
            "Epoch 590/750 | AR Loss: 0.0079\n",
            "Epoch 590/750 | AR Loss: 0.0076\n",
            "Epoch 600/750 | AR Loss: 0.0348\n",
            "Epoch 600/750 | AR Loss: 0.0277\n",
            "Epoch 600/750 | AR Loss: 0.0217\n",
            "Epoch 600/750 | AR Loss: 0.0177\n",
            "Epoch 600/750 | AR Loss: 0.0173\n",
            "Epoch 600/750 | AR Loss: 0.0153\n",
            "Epoch 600/750 | AR Loss: 0.0194\n",
            "Epoch 600/750 | AR Loss: 0.0209\n",
            "Epoch 600/750 | AR Loss: 0.0123\n",
            "Epoch 600/750 | AR Loss: 0.0130\n",
            "Epoch 600/750 | AR Loss: 0.0078\n",
            "Epoch 600/750 | AR Loss: 0.0074\n",
            "Epoch 610/750 | AR Loss: 0.0336\n",
            "Epoch 610/750 | AR Loss: 0.0264\n",
            "Epoch 610/750 | AR Loss: 0.0208\n",
            "Epoch 610/750 | AR Loss: 0.0170\n",
            "Epoch 610/750 | AR Loss: 0.0166\n",
            "Epoch 610/750 | AR Loss: 0.0147\n",
            "Epoch 610/750 | AR Loss: 0.0188\n",
            "Epoch 610/750 | AR Loss: 0.0205\n",
            "Epoch 610/750 | AR Loss: 0.0119\n",
            "Epoch 610/750 | AR Loss: 0.0124\n",
            "Epoch 610/750 | AR Loss: 0.0072\n",
            "Epoch 610/750 | AR Loss: 0.0071\n",
            "Epoch 620/750 | AR Loss: 0.0323\n",
            "Epoch 620/750 | AR Loss: 0.0253\n",
            "Epoch 620/750 | AR Loss: 0.0200\n",
            "Epoch 620/750 | AR Loss: 0.0162\n",
            "Epoch 620/750 | AR Loss: 0.0160\n",
            "Epoch 620/750 | AR Loss: 0.0141\n",
            "Epoch 620/750 | AR Loss: 0.0180\n",
            "Epoch 620/750 | AR Loss: 0.0198\n",
            "Epoch 620/750 | AR Loss: 0.0114\n",
            "Epoch 620/750 | AR Loss: 0.0120\n",
            "Epoch 620/750 | AR Loss: 0.0070\n",
            "Epoch 620/750 | AR Loss: 0.0069\n",
            "Epoch 630/750 | AR Loss: 0.0313\n",
            "Epoch 630/750 | AR Loss: 0.0245\n",
            "Epoch 630/750 | AR Loss: 0.0192\n",
            "Epoch 630/750 | AR Loss: 0.0155\n",
            "Epoch 630/750 | AR Loss: 0.0153\n",
            "Epoch 630/750 | AR Loss: 0.0135\n",
            "Epoch 630/750 | AR Loss: 0.0173\n",
            "Epoch 630/750 | AR Loss: 0.0192\n",
            "Epoch 630/750 | AR Loss: 0.0108\n",
            "Epoch 630/750 | AR Loss: 0.0116\n",
            "Epoch 630/750 | AR Loss: 0.0068\n",
            "Epoch 630/750 | AR Loss: 0.0067\n",
            "Epoch 640/750 | AR Loss: 0.0302\n",
            "Epoch 640/750 | AR Loss: 0.0235\n",
            "Epoch 640/750 | AR Loss: 0.0184\n",
            "Epoch 640/750 | AR Loss: 0.0149\n",
            "Epoch 640/750 | AR Loss: 0.0148\n",
            "Epoch 640/750 | AR Loss: 0.0129\n",
            "Epoch 640/750 | AR Loss: 0.0167\n",
            "Epoch 640/750 | AR Loss: 0.0183\n",
            "Epoch 640/750 | AR Loss: 0.0103\n",
            "Epoch 640/750 | AR Loss: 0.0111\n",
            "Epoch 640/750 | AR Loss: 0.0065\n",
            "Epoch 640/750 | AR Loss: 0.0065\n",
            "Epoch 650/750 | AR Loss: 0.0292\n",
            "Epoch 650/750 | AR Loss: 0.0226\n",
            "Epoch 650/750 | AR Loss: 0.0176\n",
            "Epoch 650/750 | AR Loss: 0.0143\n",
            "Epoch 650/750 | AR Loss: 0.0142\n",
            "Epoch 650/750 | AR Loss: 0.0124\n",
            "Epoch 650/750 | AR Loss: 0.0161\n",
            "Epoch 650/750 | AR Loss: 0.0179\n",
            "Epoch 650/750 | AR Loss: 0.0098\n",
            "Epoch 650/750 | AR Loss: 0.0107\n",
            "Epoch 650/750 | AR Loss: 0.0062\n",
            "Epoch 650/750 | AR Loss: 0.0063\n",
            "Epoch 660/750 | AR Loss: 0.0283\n",
            "Epoch 660/750 | AR Loss: 0.0216\n",
            "Epoch 660/750 | AR Loss: 0.0170\n",
            "Epoch 660/750 | AR Loss: 0.0138\n",
            "Epoch 660/750 | AR Loss: 0.0136\n",
            "Epoch 660/750 | AR Loss: 0.0119\n",
            "Epoch 660/750 | AR Loss: 0.0156\n",
            "Epoch 660/750 | AR Loss: 0.0182\n",
            "Epoch 660/750 | AR Loss: 0.0098\n",
            "Epoch 660/750 | AR Loss: 0.0099\n",
            "Epoch 660/750 | AR Loss: 0.0056\n",
            "Epoch 660/750 | AR Loss: 0.0060\n",
            "Epoch 670/750 | AR Loss: 0.0273\n",
            "Epoch 670/750 | AR Loss: 0.0207\n",
            "Epoch 670/750 | AR Loss: 0.0163\n",
            "Epoch 670/750 | AR Loss: 0.0132\n",
            "Epoch 670/750 | AR Loss: 0.0131\n",
            "Epoch 670/750 | AR Loss: 0.0114\n",
            "Epoch 670/750 | AR Loss: 0.0152\n",
            "Epoch 670/750 | AR Loss: 0.0174\n",
            "Epoch 670/750 | AR Loss: 0.0091\n",
            "Epoch 670/750 | AR Loss: 0.0094\n",
            "Epoch 670/750 | AR Loss: 0.0054\n",
            "Epoch 670/750 | AR Loss: 0.0058\n",
            "Epoch 680/750 | AR Loss: 0.0264\n",
            "Epoch 680/750 | AR Loss: 0.0199\n",
            "Epoch 680/750 | AR Loss: 0.0157\n",
            "Epoch 680/750 | AR Loss: 0.0127\n",
            "Epoch 680/750 | AR Loss: 0.0125\n",
            "Epoch 680/750 | AR Loss: 0.0110\n",
            "Epoch 680/750 | AR Loss: 0.0146\n",
            "Epoch 680/750 | AR Loss: 0.0161\n",
            "Epoch 680/750 | AR Loss: 0.0083\n",
            "Epoch 680/750 | AR Loss: 0.0093\n",
            "Epoch 680/750 | AR Loss: 0.0054\n",
            "Epoch 680/750 | AR Loss: 0.0057\n",
            "Epoch 690/750 | AR Loss: 0.0255\n",
            "Epoch 690/750 | AR Loss: 0.0191\n",
            "Epoch 690/750 | AR Loss: 0.0152\n",
            "Epoch 690/750 | AR Loss: 0.0123\n",
            "Epoch 690/750 | AR Loss: 0.0120\n",
            "Epoch 690/750 | AR Loss: 0.0105\n",
            "Epoch 690/750 | AR Loss: 0.0141\n",
            "Epoch 690/750 | AR Loss: 0.0155\n",
            "Epoch 690/750 | AR Loss: 0.0079\n",
            "Epoch 690/750 | AR Loss: 0.0089\n",
            "Epoch 690/750 | AR Loss: 0.0051\n",
            "Epoch 690/750 | AR Loss: 0.0055\n",
            "Epoch 700/750 | AR Loss: 0.0247\n",
            "Epoch 700/750 | AR Loss: 0.0185\n",
            "Epoch 700/750 | AR Loss: 0.0147\n",
            "Epoch 700/750 | AR Loss: 0.0119\n",
            "Epoch 700/750 | AR Loss: 0.0116\n",
            "Epoch 700/750 | AR Loss: 0.0102\n",
            "Epoch 700/750 | AR Loss: 0.0135\n",
            "Epoch 700/750 | AR Loss: 0.0148\n",
            "Epoch 700/750 | AR Loss: 0.0075\n",
            "Epoch 700/750 | AR Loss: 0.0085\n",
            "Epoch 700/750 | AR Loss: 0.0049\n",
            "Epoch 700/750 | AR Loss: 0.0053\n",
            "Epoch 710/750 | AR Loss: 0.0239\n",
            "Epoch 710/750 | AR Loss: 0.0180\n",
            "Epoch 710/750 | AR Loss: 0.0142\n",
            "Epoch 710/750 | AR Loss: 0.0115\n",
            "Epoch 710/750 | AR Loss: 0.0112\n",
            "Epoch 710/750 | AR Loss: 0.0098\n",
            "Epoch 710/750 | AR Loss: 0.0130\n",
            "Epoch 710/750 | AR Loss: 0.0142\n",
            "Epoch 710/750 | AR Loss: 0.0072\n",
            "Epoch 710/750 | AR Loss: 0.0083\n",
            "Epoch 710/750 | AR Loss: 0.0048\n",
            "Epoch 710/750 | AR Loss: 0.0052\n",
            "Epoch 720/750 | AR Loss: 0.0232\n",
            "Epoch 720/750 | AR Loss: 0.0174\n",
            "Epoch 720/750 | AR Loss: 0.0138\n",
            "Epoch 720/750 | AR Loss: 0.0111\n",
            "Epoch 720/750 | AR Loss: 0.0109\n",
            "Epoch 720/750 | AR Loss: 0.0095\n",
            "Epoch 720/750 | AR Loss: 0.0126\n",
            "Epoch 720/750 | AR Loss: 0.0138\n",
            "Epoch 720/750 | AR Loss: 0.0069\n",
            "Epoch 720/750 | AR Loss: 0.0079\n",
            "Epoch 720/750 | AR Loss: 0.0045\n",
            "Epoch 720/750 | AR Loss: 0.0050\n",
            "Epoch 730/750 | AR Loss: 0.0225\n",
            "Epoch 730/750 | AR Loss: 0.0167\n",
            "Epoch 730/750 | AR Loss: 0.0134\n",
            "Epoch 730/750 | AR Loss: 0.0108\n",
            "Epoch 730/750 | AR Loss: 0.0106\n",
            "Epoch 730/750 | AR Loss: 0.0092\n",
            "Epoch 730/750 | AR Loss: 0.0122\n",
            "Epoch 730/750 | AR Loss: 0.0134\n",
            "Epoch 730/750 | AR Loss: 0.0066\n",
            "Epoch 730/750 | AR Loss: 0.0075\n",
            "Epoch 730/750 | AR Loss: 0.0044\n",
            "Epoch 730/750 | AR Loss: 0.0048\n",
            "Epoch 740/750 | AR Loss: 0.0218\n",
            "Epoch 740/750 | AR Loss: 0.0162\n",
            "Epoch 740/750 | AR Loss: 0.0129\n",
            "Epoch 740/750 | AR Loss: 0.0105\n",
            "Epoch 740/750 | AR Loss: 0.0102\n",
            "Epoch 740/750 | AR Loss: 0.0089\n",
            "Epoch 740/750 | AR Loss: 0.0117\n",
            "Epoch 740/750 | AR Loss: 0.0125\n",
            "Epoch 740/750 | AR Loss: 0.0063\n",
            "Epoch 740/750 | AR Loss: 0.0074\n",
            "Epoch 740/750 | AR Loss: 0.0042\n",
            "Epoch 740/750 | AR Loss: 0.0047\n",
            "Epoch 750/750 | AR Loss: 0.0212\n",
            "Epoch 750/750 | AR Loss: 0.0157\n",
            "Epoch 750/750 | AR Loss: 0.0126\n",
            "Epoch 750/750 | AR Loss: 0.0102\n",
            "Epoch 750/750 | AR Loss: 0.0099\n",
            "Epoch 750/750 | AR Loss: 0.0086\n",
            "Epoch 750/750 | AR Loss: 0.0113\n",
            "Epoch 750/750 | AR Loss: 0.0118\n",
            "Epoch 750/750 | AR Loss: 0.0061\n",
            "Epoch 750/750 | AR Loss: 0.0071\n",
            "Epoch 750/750 | AR Loss: 0.0041\n",
            "Epoch 750/750 | AR Loss: 0.0045\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Total iterations: 1675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the model AutoRegressively"
      ],
      "metadata": {
        "id": "rkRq8gYolAh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def inference(test_input):\n",
        "  '''\n",
        "  Run the model in autoregressive fashion and store the output at each time step in a list\n",
        "  '''\n",
        "\n",
        "  tar_token_ids = torch.zeros((1, dec_ctxt_len), dtype=torch.int)\n",
        "  tar_token_ids[:, 0] = 1\n",
        "  for item in range(dec_ctxt_len - 1):\n",
        "    out = model(test_input.unsqueeze(0), tar_token_ids)\n",
        "    pred = torch.argmax(F.softmax(out, dim=-1), dim=-1)\n",
        "    tar_token_ids[:, item + 1] = pred[:, item]\n",
        "  return tar_token_ids.squeeze(0)\n"
      ],
      "metadata": {
        "id": "psLM9R44lEMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Modify the code below to suit your implementation\n",
        "* Display the original and translated sentence (with all the spcial tokens)\n",
        "* Note that, the second half of the second sentence is poorly translated\n",
        "*  Same goes for 3rd and 4th sentence\n",
        "* All other sentences are properly translated"
      ],
      "metadata": {
        "id": "i3GljM0lkgzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_ids in x:\n",
        "  print(src_tokenizer.decode(token_ids))\n",
        "  print(tar_tokenizer.decode(inference(token_ids)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb04_GlqYaMj",
        "outputId": "2dd468ad-ae58-4ae0-e3df-8b5907df7838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> The most famous ruler of ancient India was Emperor Ashoka. <end> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "<start> பண்டைய இந்திய அரசர்களில் பேரும் புகழும் பெற்ற அரசர் அசோகர் ஆவார். <end> <pad> <pad>\n",
            "<start> It was during his period that Buddhism spread to different parts of Asia. <end> <pad> <pad> <pad>\n",
            "<start> இவரது ஆட்சியில் தான் புத்த மதம் ஆசியாவின் பல்வேறு பகுதிகளுக்குப் பரவியது. <end> <pad> <pad>\n",
            "<start> Ashoka gave up war after seeing many people grieving death after the Kalinga war. <end> <pad> <pad>\n",
            "<start> கலிங்கப் போருக்குப் பின் பல உயிர்கள் மடிவதைக் கண்டு வருந்தி, போர் தொடுப்பதைக் கைவிட்டார். <end>\n",
            "<start> He embraced Buddhism and then devoted his life to spread the message of peace and dharma. <end>\n",
            "<start> அதற்குப் பிறகு புத்த சமயத்தைத் தழுவி, அமைதியையும் அறத்தையும் பரப்புவதற்காகத் தன் வாழ்வையே அர்ப்பணித்தார். <end>\n",
            "<start> His service for the cause of public good was exemplary. <end> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "<start> பொதுமக்களுக்கு அவர் ஆற்றிய சேவை முன் மாதிரியாக விளங்கியது. <end> <pad> <pad> <pad> <pad>\n",
            "<start> He was the first ruler to give up war after victory. <end> <pad> <pad> <pad> <pad> <pad>\n",
            "<start> வெற்றிக்குப் பின் போரைத் துறந்த முதல் அரசர் அசோகர்தான். <end> <pad> <pad> <pad> <pad>\n",
            "<start> He was the first to build hospitals for animals. <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "<start> உலகிலேயே முதன்முதலாக விலங்குகளுக்கும் தனியே மருத்துவமனை அமைத்துத் தந்தவரும் அசோகரே ஆவார். <end> <pad> <pad>\n",
            "<start> He was the first to lay roads. <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "<start>  இன்றும் அவர் உருவாக்கிய சாலைகளை நாம் பயன்படுத்திக்கொண்டு இருக்கிறோம். <end> <pad> <pad> <pad>\n"
          ]
        }
      ]
    }
  ]
}