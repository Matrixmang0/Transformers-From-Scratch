{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* In this assignment you will be building the **Encoder** part of the Transformer architecture.\n",
        "* You will be using the **PyTorch** framework to implement the following components\n",
        "  * Encoder Layer that contains\n",
        "    * Multi-Head Attention (MHA) Module\n",
        "    * Position-wise Feed Forward Neural Network\n",
        "\n",
        "  * Output layer that takes the encoder output and predicts the token_ids.\n",
        "\n",
        "  * Optionally, study whether adding positional information is helpful.\n",
        "  \n",
        "* **DO NOT** USE Built-in **TRANSFORMER LAYERS** as it affects the reproducibility.\n",
        "\n",
        "* You will be given with a configuration file that contains information on various hyperparameters such as embedding dimension, vocabulary size,number heads and so on\n",
        "\n",
        "* Use ReLU activation function and Stochastic Gradient Descent optimizer\n",
        "* Here are a list of helpful Pytorch functions (does not mean you have to use all of them) for this and subsequent assignments\n",
        "  * [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch-matmul)\n",
        "  * [torch.bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "  * torch.swapdims\n",
        "  * torch.unsqueeze\n",
        "  * torch.squeeze\n",
        "  * torch.argmax\n",
        "  * [torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)\n",
        "  * [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "  * [torch.nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)\n",
        "  * torch.nn.Linear\n",
        "  * torch.nn.LayerNorm\n",
        "  * torch.nn.ModuleList\n",
        "  * torch.nn.Sequential\n",
        "  * [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "  \n",
        "* Important: **Do not** set any global seeds.\n",
        "\n",
        "* Helpful resources to get started with\n",
        "\n",
        " * [Annotated Transformers](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
        " * [PyTorch Source code of Transformer Layer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "3BzlkwtBUSLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "xp0X3WMuWEYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from  pprint import pprint\n",
        "from yaml import safe_load\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import math"
      ],
      "metadata": {
        "id": "OR-MhDgVWMYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "WbHXnQ2WWlHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_url = \"https://raw.githubusercontent.com/Arunprakash-A/LLM-from-scratch-PyTorch/main/config_files/enc_config.yml\"\n",
        "response = requests.get(config_url)\n",
        "config = response.content.decode(\"utf-8\")\n",
        "config = safe_load(config)\n",
        "pprint(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GaFbEI1WnBD",
        "outputId": "59324a7b-cd87-44a7-f6fc-9de58ac43ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': {'batch_size': 10, 'embed_dim': 32, 'seq_len': 8, 'vocab_size': 10},\n",
            " 'model': {'d_ff': 128,\n",
            "           'd_model': 32,\n",
            "           'dk': 4,\n",
            "           'dq': 4,\n",
            "           'dv': 4,\n",
            "           'n_heads': 8,\n",
            "           'n_layers': 6}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = config['input']['vocab_size']\n",
        "batch_size = config['input']['batch_size']\n",
        "seq_len = config['input']['seq_len']\n",
        "embed_dim = config['input']['embed_dim']"
      ],
      "metadata": {
        "id": "G5I_iBP7XZod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary Size :\", vocab_size)\n",
        "print(\"Batch Size :\", batch_size)\n",
        "print(\"Sequence Length :\", seq_len)\n",
        "print(\"Embedding Dimension :\", embed_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfvB0znzljIa",
        "outputId": "a9db7dff-6764-4ed8-9b60-03db1e66880d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size : 10\n",
            "Batch Size : 10\n",
            "Sequence Length : 8\n",
            "Embedding Dimension : 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here, you are directly given with the token ids\n",
        "* Assume that length of all sequences are equal to the context length (T) (so that we do not need to bother about padding shorter sequences while batching)"
      ],
      "metadata": {
        "id": "iHswIewIX5aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://github.com/Arunprakash-A/LLM-from-scratch-PyTorch/raw/main/config_files/w1_input_tokens'\n",
        "r = requests.get(data_url)\n",
        "token_ids = torch.load(BytesIO(r.content))\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaawBcu3a3jX",
        "outputId": "15991e19-b31c-4db5-892a-39a70e22678d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5, 7, 5, 6, 3, 8, 7, 5],\n",
            "        [7, 2, 7, 1, 2, 1, 1, 7],\n",
            "        [1, 0, 0, 3, 6, 3, 0, 8],\n",
            "        [5, 0, 2, 8, 6, 5, 5, 3],\n",
            "        [3, 5, 4, 8, 5, 0, 7, 3],\n",
            "        [8, 6, 7, 4, 4, 4, 0, 1],\n",
            "        [5, 8, 1, 0, 1, 1, 0, 3],\n",
            "        [1, 7, 8, 8, 0, 5, 3, 7],\n",
            "        [7, 7, 1, 4, 5, 6, 7, 0],\n",
            "        [1, 7, 2, 8, 3, 0, 0, 4]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-0fefa6fcf57f>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  token_ids = torch.load(BytesIO(r.content))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the sub-layers"
      ],
      "metadata": {
        "id": "ZhMRsvQvZh6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dq = torch.tensor(config['model']['dq'])\n",
        "dk = torch.tensor(config['model']['dk'])\n",
        "dv = torch.tensor(config['model']['dv'])\n",
        "dmodel = embed_dim\n",
        "heads = torch.tensor(config['model']['n_heads'])\n",
        "d_ff = config['model']['d_ff']"
      ],
      "metadata": {
        "id": "29nA7XtsZn4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Head Attention\n",
        "\n",
        " * Be mindful when using `torch.matmul`\n",
        " * Ensure that you understood what is being computed (because matrix product is not commutative)\n",
        " * Randomly initialize the parameters using normal distribution with the following seed values\n",
        "  * $W_Q:$(seed=43)\n",
        "  * $W_K:$(seed=44)\n",
        "  * $W_V:$(seed=45)\n",
        "  * $W_O:$(seed=46)"
      ],
      "metadata": {
        "id": "vxyfd2nfZ3on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,heads):\n",
        "    super(MHA,self).__init__()\n",
        "\n",
        "    self.dq = dq\n",
        "    self.dk = dk\n",
        "    self.dv = dv\n",
        "    self.dmodel = dmodel\n",
        "    self.heads = heads\n",
        "\n",
        "    torch.manual_seed(43)\n",
        "    self.W_q = nn.Parameter(torch.randn(heads, dmodel, dq))\n",
        "\n",
        "    torch.manual_seed(44)\n",
        "    self.W_k = nn.Parameter(torch.randn(heads, dmodel, dk))\n",
        "\n",
        "    torch.manual_seed(45)\n",
        "    self.W_v = nn.Parameter(torch.randn(heads, dmodel, dv))\n",
        "\n",
        "    torch.manual_seed(46)\n",
        "    self.W_o = nn.Parameter(torch.randn(dmodel, dmodel))\n",
        "\n",
        "  def forward(self,H=None):\n",
        "    '''\n",
        "    Input: Size [BSxTxdmodel]\n",
        "    Output: Size[BSxTxdmodel]\n",
        "    '''\n",
        "\n",
        "    BS, T, dmodel = H.size()\n",
        "\n",
        "    Q = torch.matmul(H.unsqueeze(1), self.W_q)\n",
        "    K = torch.matmul(H.unsqueeze(1), self.W_k)\n",
        "    V = torch.matmul(H.unsqueeze(1), self.W_v)\n",
        "\n",
        "    attention_score = torch.matmul(Q, K.transpose(2,3))/math.sqrt(self.dk)\n",
        "    attention_score = torch.softmax(attention_score, dim = -1)\n",
        "\n",
        "    z = torch.matmul(attention_score, V)\n",
        "    z = z.permute(0,2,1,3).contiguous()\n",
        "    z = z.view(BS, T, -1)\n",
        "\n",
        "    out = torch.matmul(z, self.W_o)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "GEihgqiTZ0E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pointwise FFN\n",
        "\n",
        "* Randomly initialize the parameters using normal distribution with the following seed values\n",
        "  * $W_{1}:$(seed=47)\n",
        "  * $W_2:$(seed=48)  "
      ],
      "metadata": {
        "id": "B7XgQNSRwO0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,d_ff,layer=0):\n",
        "    super(FFN,self).__init__()\n",
        "\n",
        "    self.dmodel = dmodel\n",
        "    self.d_ff = d_ff\n",
        "\n",
        "    torch.manual_seed(47)\n",
        "    self.W_1 = nn.Parameter(torch.randn(dmodel, d_ff))\n",
        "\n",
        "    torch.manual_seed(48)\n",
        "    self.W_2 = nn.Parameter(torch.randn(d_ff, dmodel))\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    '''\n",
        "    input: size [BSxTxdmodel]\n",
        "    output: size [BSxTxdmodel]\n",
        "    '''\n",
        "\n",
        "    out = torch.matmul(x, self.W_1)\n",
        "\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = torch.matmul(out, self.W_2)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "InQxHsqUv9b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Layer\n",
        "\n",
        "* Randomly initialize the linear layer\n",
        " * $W_L:$(seed=49)\n"
      ],
      "metadata": {
        "id": "Y2-ALRSRwVxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,vocab_size):\n",
        "    super(OutputLayer,self).__init__()\n",
        "\n",
        "    torch.manual_seed(49)\n",
        "    self.W_L = nn.Parameter(torch.randn(dmodel, vocab_size))\n",
        "\n",
        "  def forward(self,representations):\n",
        "    '''\n",
        "    input: size [bsxTxdmodel]\n",
        "    output: size [bsxTxvocab_size]\n",
        "    Note: Do not apply the softmax. Just return the output of linear transformation\n",
        "    '''\n",
        "    out = torch.matmul(representations, self.W_L)\n",
        "    return out"
      ],
      "metadata": {
        "id": "25yaAbKHwXNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Layer"
      ],
      "metadata": {
        "id": "7bD8YU5Ww2eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,d_ff,heads):\n",
        "    super(EncoderLayer,self).__init__()\n",
        "    self.mha = MHA(dmodel,dq,dk,dv,heads)\n",
        "    self.layer_norm_mha = torch.nn.LayerNorm(dmodel)\n",
        "    self.layer_norm_ffn = torch.nn.LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,d_ff)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    out_1 = self.mha(x)\n",
        "\n",
        "    out_1 = self.layer_norm_mha(out_1+x)\n",
        "\n",
        "    out = self.ffn(out_1)\n",
        "\n",
        "    out = self.layer_norm_ffn(out+out_1)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "gCHDYAKWwz4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model with one encoder layer\n",
        "\n",
        " * The encoders' forward function accepts the token_ids as input\n",
        " * Generate the embeddings for the token ids by initializing the emebedding weights from normal distribution by setting the seed value to 50\n",
        " * Use `torch.nn.Embed()` to generate required embeddings"
      ],
      "metadata": {
        "id": "wrZ84eSyxYfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,embed_dim,dq,dk,dv,d_ff,heads,num_layers=1):\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    torch.manual_seed(50)\n",
        "    self.embed_weights = nn.Parameter(torch.randn(vocab_size, embed_dim))\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, _weight=self.embed_weights)\n",
        "\n",
        "    self.encoder_layer = EncoderLayer(embed_dim, dq, dk, dv, d_ff, heads)\n",
        "    self.output_layer = OutputLayer(embed_dim, vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    '''\n",
        "    The input should be tokens ids of size [BS,T]\n",
        "    '''\n",
        "    out = self.embed(x) # get the embeddings of the tokens\n",
        "    out = self.encoder_layer(out) # pass the embeddings throught the encoder layers\n",
        "    out = self.output_layer(out) # get the logits\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "WctNu-Z-w5rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Encoder(vocab_size,dmodel,dq,dk,dv,d_ff,heads)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "owqyMc_Fxghn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "\n",
        " * Train the model for 30 epochs and compute the loss"
      ],
      "metadata": {
        "id": "Wmgf_oYl6hr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(token_ids, model, optimizer, criterion, epochs=None):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    out = model(token_ids)\n",
        "\n",
        "    batch_size, seq_len, vocab_size = out.size()\n",
        "    out = out.view(-1, vocab_size)\n",
        "    target = token_ids.view(-1)\n",
        "\n",
        "    loss = criterion(out, target)\n",
        "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "Ebgg-hwGxxv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(token_ids, model, optimizer, criterion, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLjYzn2Ax3TU",
        "outputId": "2309e4d1-f9f9-44b3-f2bd-dfb70a6d0b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 10.118653297424316\n",
            "Epoch 1, Loss: 8.861600875854492\n",
            "Epoch 2, Loss: 8.083215713500977\n",
            "Epoch 3, Loss: 7.3580474853515625\n",
            "Epoch 4, Loss: 6.783898830413818\n",
            "Epoch 5, Loss: 6.39406681060791\n",
            "Epoch 6, Loss: 6.037106990814209\n",
            "Epoch 7, Loss: 5.667043209075928\n",
            "Epoch 8, Loss: 5.352385520935059\n",
            "Epoch 9, Loss: 5.088061332702637\n",
            "Epoch 10, Loss: 4.846492290496826\n",
            "Epoch 11, Loss: 4.6231794357299805\n",
            "Epoch 12, Loss: 4.4200358390808105\n",
            "Epoch 13, Loss: 4.247159004211426\n",
            "Epoch 14, Loss: 4.019876003265381\n",
            "Epoch 15, Loss: 3.756844997406006\n",
            "Epoch 16, Loss: 3.4984729290008545\n",
            "Epoch 17, Loss: 3.398616313934326\n",
            "Epoch 18, Loss: 3.2541491985321045\n",
            "Epoch 19, Loss: 3.1784861087799072\n",
            "Epoch 20, Loss: 3.0574450492858887\n",
            "Epoch 21, Loss: 2.99891996383667\n",
            "Epoch 22, Loss: 2.886470317840576\n",
            "Epoch 23, Loss: 2.8377766609191895\n",
            "Epoch 24, Loss: 2.7386815547943115\n",
            "Epoch 25, Loss: 2.690978765487671\n",
            "Epoch 26, Loss: 2.6101996898651123\n",
            "Epoch 27, Loss: 2.5524134635925293\n",
            "Epoch 28, Loss: 2.4962239265441895\n",
            "Epoch 29, Loss: 2.4203381538391113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "WE7gk3Wj6nyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  predictions =  model(token_ids) # predict the labels\n",
        "  predicted_labels = predictions.argmax(dim=-1)"
      ],
      "metadata": {
        "id": "hYSG8_HCyGbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* See how many labels are correctly predicted"
      ],
      "metadata": {
        "id": "ORsnaC9L64U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.count_nonzero(token_ids==predicted_labels).item())"
      ],
      "metadata": {
        "id": "w1EODJB3yJ1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed665947-7a9a-4a18-a0e6-aee8b49e6acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The loss by now should be about 2.39 and the number of correct predictions should be about 37"
      ],
      "metadata": {
        "id": "4ifS0e81G2Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder with N Layers\n",
        "\n",
        "  * The intialized parameters in all layers are identical\n",
        "  * use ModuleList to create **deep-copies** of encoder layer"
      ],
      "metadata": {
        "id": "hZm-fFLRyTRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy"
      ],
      "metadata": {
        "id": "S2k-q-HJzASs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,dmodel,dq,dk,dv,d_ff,heads,num_layers=1):\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "    self.embed_weights = nn.Parameter(torch.randn(vocab_size, embed_dim))\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, _weight=self.embed_weights)\n",
        "\n",
        "    enc_layer = EncoderLayer(dmodel, dq, dk, dv, d_ff, heads)\n",
        "    self.enc_layers = nn.ModuleList([copy.deepcopy(enc_layer) for _ in range(num_layers)])\n",
        "\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    self.output_layer = OutputLayer(dmodel, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    '''\n",
        "    1. Get embeddings\n",
        "    2. Pass it through encoder layer-1 and recursively pass the output to subsequent enc.layers\n",
        "    3. output the logits\n",
        "    '''\n",
        "\n",
        "    out = self.embed(x)\n",
        "\n",
        "    out = self.norm(out)\n",
        "\n",
        "    for i, layer in enumerate(self.enc_layers):\n",
        "            out = layer(out)\n",
        "\n",
        "    out = self.output_layer(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "4c3LdS8AyU0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train the stack of encoder layers with `num_layers=2` for the same 30 epochs"
      ],
      "metadata": {
        "id": "7E4xZxWi8XtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Encoder(vocab_size,dmodel,dq,dk,dv,d_ff,heads,num_layers=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "ErrBXMf_zL5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(token_ids, model, optimizer, criterion, epochs=30)"
      ],
      "metadata": {
        "id": "BeCyhBBmzY5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9e30e5-a24c-4e45-d57a-16fa5f088609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 12.377585411071777\n",
            "Epoch 1, Loss: 10.060522079467773\n",
            "Epoch 2, Loss: 8.443035125732422\n",
            "Epoch 3, Loss: 7.300762176513672\n",
            "Epoch 4, Loss: 5.969535827636719\n",
            "Epoch 5, Loss: 5.122201442718506\n",
            "Epoch 6, Loss: 4.611206531524658\n",
            "Epoch 7, Loss: 4.359195709228516\n",
            "Epoch 8, Loss: 3.807462692260742\n",
            "Epoch 9, Loss: 3.5612869262695312\n",
            "Epoch 10, Loss: 3.303182601928711\n",
            "Epoch 11, Loss: 3.124396324157715\n",
            "Epoch 12, Loss: 2.9949800968170166\n",
            "Epoch 13, Loss: 2.8886213302612305\n",
            "Epoch 14, Loss: 2.7892909049987793\n",
            "Epoch 15, Loss: 2.6888840198516846\n",
            "Epoch 16, Loss: 2.624729633331299\n",
            "Epoch 17, Loss: 2.5725009441375732\n",
            "Epoch 18, Loss: 2.5149991512298584\n",
            "Epoch 19, Loss: 2.4602086544036865\n",
            "Epoch 20, Loss: 2.411038875579834\n",
            "Epoch 21, Loss: 2.3677639961242676\n",
            "Epoch 22, Loss: 2.331463098526001\n",
            "Epoch 23, Loss: 2.246312379837036\n",
            "Epoch 24, Loss: 2.210348129272461\n",
            "Epoch 25, Loss: 2.1535091400146484\n",
            "Epoch 26, Loss: 2.1113452911376953\n",
            "Epoch 27, Loss: 2.0613067150115967\n",
            "Epoch 28, Loss: 2.0267081260681152\n",
            "Epoch 29, Loss: 1.9786207675933838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  predictions =  model(token_ids) # predict the labels\n",
        "  predicted_labels = predictions.argmax(dim=-1)"
      ],
      "metadata": {
        "id": "lWl0YGbI0Dy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.count_nonzero(predicted_labels==token_ids).item()"
      ],
      "metadata": {
        "id": "llZyhTY20JOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd298776-892c-4389-d290-a283268a8c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now, the loss value should be about 1.9 and the number of correct preditions is about 38"
      ],
      "metadata": {
        "id": "aZkfYjpnLwax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Number of Parameters"
      ],
      "metadata": {
        "id": "YJHxAYffFlVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_num_parameters = 0\n",
        "\n",
        "for parameter in model.parameters():\n",
        "  total_num_parameters += parameter.numel()\n",
        "\n",
        "print('total number of parameters in the model \\n including the embedding layer is:', total_num_parameters)"
      ],
      "metadata": {
        "id": "7xrviY4PFoHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f533fc-98db-4130-ef82-025deaeb6670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of parameters in the model \n",
            " including the embedding layer is: 25856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Positional Encoding\n",
        "\n",
        " * We now use the positional embedding as defined in the [paper](https://arxiv.org/pdf/1706.03762v1.pdf) (differs a bit from the lecture).\n",
        " * Note that, the positional encoding for each position is fixed (not a learnable parameter)\n",
        " * However, we add this with the raw_embeddings which are learnable.\n",
        " * Therefore, it is important to create a class definition for PE and register PE parameters in the buffer (in case we move the model to GPU)\n",
        " * Just create a matrix of same size of input and add it to the embeddings"
      ],
      "metadata": {
        "id": "EVZhnRymGU4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self,d_model,max_len=8):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        #compute it in the log space\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add positional embedding\n",
        "\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "kMyJFoEpFqnR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}